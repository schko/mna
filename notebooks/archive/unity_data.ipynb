{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96671aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analyze sessions in batch from Phase 1 of AdaDrive (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "261d0746",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "# setting path\n",
    "sys.path.append('..')\n",
    "\n",
    "from mna.utils.rnapp_data_format import read_all_lslpresets, return_metadata_from_name, event_data_from_data\n",
    "import pickle, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mna.sessions.eye_session import process_session_eye\n",
    "from mna.sessions.eeg_session import process_session_eeg\n",
    "from mna.sessions.motor_session import process_session_motor\n",
    "from mna.sessions.ecg_session import process_session_ecg\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from mna.utils.rnapp_data_format import read_all_lslpresets, return_metadata_from_name, event_data_from_data\n",
    "import pickle\n",
    "from statannotations.Annotator import Annotator\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import mne\n",
    "import glob \n",
    "import random\n",
    "import math\n",
    "# 1. Read a RN App, converted pkl file, and create the metadata and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9654537",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13166b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d3255b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mna.utils.rnapp_data_format import read_all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4865e6a1-51db-4e99-a973-f1421e0b648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "lsl_dir = \"../mna/LSLPresets/\"\n",
    "output_dir = '../output/batch_analysis/'\n",
    "if not os.path.isdir(output_dir): os.makedirs(output_dir)\n",
    "metadata_jsons = read_all_lslpresets(path_to_jsonfiles=lsl_dir)\n",
    "onlyfiles = [f for f in listdir(data_dir) if isfile(join(data_dir, f)) and '.pkl' in f]\n",
    "\n",
    "interrupted_sessions = [(13,1), (22,1)]\n",
    "reference_ica = \"sbj20ssn03\"\n",
    "save_data_pkl = True # save data into pickle files\n",
    "save_ica_plts = False # save ICA components plots\n",
    "epoch_raw_eeg = False # epoching raw data\n",
    "motor_events = True\n",
    "preturn = 1000\n",
    "rs = 64 # random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0fa1af-b6a6-4af6-bcf7-ffb47dae94ee",
   "metadata": {},
   "source": [
    "# Motor-based break detection (11/16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae7121-9c7f-4ae0-9a9b-6a38c549e7e2",
   "metadata": {},
   "source": [
    "### Export re-referenced raw event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daca1c39-03b7-401a-b6a5-ce8fc020fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_metadata_from_name(stream_name, metadata_jsons):\n",
    "    for stream in metadata_jsons:\n",
    "        if stream['StreamName'] == stream_name or stream['StreamName'] == stream_name.replace(\"_\", \".\"):\n",
    "            return stream\n",
    "    return None\n",
    "def add_trial_start_time(event_df, offset=0.01):\n",
    "    trial_end_times = np.zeros(event_df.shape[0])\n",
    "    trial_end_times[1:] = event_df.trial_end_time[0:-1]+offset # add a 0.01 second offset since the next trial starts immediately\n",
    "    event_df.insert(0, \"trial_start_time\", trial_end_times)\n",
    "interrupted_id_sessions = [(13,1), (22,1)]\n",
    "all_dfs = []\n",
    "#for each_file in onlyfiles:\n",
    "for each_file in ['09_09_2022_14_24_33-Exp_adadrive-Sbj_17-Ssn_01.dats.pkl']:   \n",
    "    input_path = data_dir + each_file\n",
    "\n",
    "    sbj_id = each_file[each_file.find('Sbj_')+4:each_file.find('-Ssn')]\n",
    "    ssn_no = each_file[each_file.find('Ssn_')+4:each_file.find('.dats')]\n",
    "    \n",
    "    if len(sbj_id) < 2: sbj = \"sbj0\"+sbj_id\n",
    "    else: sbj = \"sbj\"+sbj_id\n",
    "    if len(ssn_no) < 2: ssn = \"ssn0\"+ssn_no\n",
    "    else: ssn = \"ssn\"+ssn_no\n",
    "\n",
    "    with open(input_path, 'rb') as handle:\n",
    "        rns_data = pickle.load(handle)\n",
    "    \n",
    "    for key in rns_data.keys():\n",
    "        rns_data[key].append(return_metadata_from_name(key, metadata_jsons))\n",
    "        \n",
    "    ## Add metadata to data\n",
    "    event_df = pd.DataFrame(rns_data['Unity_TrialInfo'][0], columns=rns_data['Unity_TrialInfo'][1],\n",
    "                  index=rns_data['Unity_TrialInfo'][2]['ChannelNames']).T\n",
    "    event_df = event_df.reset_index().rename(columns={'index': 'trial_end_time'})\n",
    "    interrupted_ids = [p[0] for p in interrupted_id_sessions]\n",
    "    interrupted_sessions = [p[1] for p in interrupted_id_sessions]\n",
    "    \n",
    "    # re-reference\n",
    "    session_start_time = rns_data['Unity_MotorInput'][1][0]\n",
    "    event_df.trial_end_time -= session_start_time\n",
    "    \n",
    "    # chunk data is always paired but offset\n",
    "    if 'Unity_ChunkInfo' in rns_data:\n",
    "        chunk_df = pd.DataFrame(rns_data['Unity_ChunkInfo'][0], columns=rns_data['Unity_ChunkInfo'][1],\n",
    "                              index=rns_data['Unity_ChunkInfo'][2]['ChannelNames']).T\n",
    "        chunk_df = chunk_df.reset_index().rename(columns={'index': 'chunk_timestamp'})\n",
    "        event_df = pd.concat([event_df,chunk_df],axis=1)\n",
    "        #chunk_df['chunk_timestamp'] = chunk_df.index\n",
    "        #event_df = pd.merge_asof(event_df, chunk_df,\n",
    "        #                         left_on=\"trial_start_time\",right_index=True,\n",
    "        #                         direction='nearest', tolerance=1)\n",
    "    else:\n",
    "        print(f\"Unity_ChunkInfo not found\")\n",
    "        \n",
    "    if event_df.iloc[0].ppid in interrupted_ids and event_df.iloc[0].session in interrupted_sessions:\n",
    "        print('FIXING THE EVENT DF SINCE PID', event_df.iloc[0].ppid, 'SESSION', event_df.iloc[0].session, 'WAS INTERRUPTED')\n",
    "        event_df = event_df.loc[~event_df.duplicated(subset=['ppid','session','block','number_in_block','trial'], keep='first'),:].reset_index(drop=True)\n",
    "        last_freak_idx = event_df.loc[event_df.ppid == 0].index[-1]\n",
    "        event_df = event_df[event_df.ppid != 0].reset_index(drop=True)\n",
    "        event_df.loc[last_freak_idx:,'session'] = event_df.loc[last_freak_idx-1,'session']\n",
    "        event_df.loc[last_freak_idx:,'block'] = event_df.loc[last_freak_idx:,'block'] + event_df.loc[last_freak_idx-1,'block']\n",
    "        event_df.loc[last_freak_idx:,'trial'] = event_df.loc[last_freak_idx:,'trial'] + event_df.loc[last_freak_idx-1,'trial']\n",
    "        event_df.loc[last_freak_idx:,'damage'] = event_df.loc[last_freak_idx:,'damage'] + event_df.loc[last_freak_idx-1,'damage']\n",
    "        \n",
    "    add_trial_start_time(event_df)\n",
    "    all_dfs.append(event_df)\n",
    "\n",
    "#pd.concat(all_dfs).to_csv('all_raw_trial_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b09693-fcb5-4d96-82ba-697b2177186e",
   "metadata": {},
   "source": [
    "### Find breaks using motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "326e1cc0-a82d-48b2-82b0-2f07d7f6aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_path ../data/09_09_2022_14_24_33-Exp_adadrive-Sbj_17-Ssn_01.dats.pkl\n"
     ]
    }
   ],
   "source": [
    "def find_breaks(nums, timestamps, break_time=5, inactive_value=0):\n",
    "    start_intervals=[]\n",
    "    end_intervals=[]\n",
    "    inactive_duration = []\n",
    "    l=timestamps[0]\n",
    "    for i in range(1,len(nums)):\n",
    "        if nums[i] != nums[i-1] and nums[i-1]==inactive_value and timestamps[i-1]-l > break_time:\n",
    "            start_intervals.append(l)\n",
    "            end_intervals.append(timestamps[i-1])\n",
    "            inactive_duration.append(timestamps[i-1]-l)\n",
    "            nums[i] != inactive_value\n",
    "        if nums[i] != inactive_value:\n",
    "            l=timestamps[i]\n",
    "    return start_intervals, end_intervals, inactive_duration\n",
    "all_dfs = []\n",
    "for each_file in onlyfiles:\n",
    "    input_path = data_dir + each_file\n",
    "    print('input_path',input_path)\n",
    "    sbj_id = each_file[each_file.find('Sbj_')+4:each_file.find('-Ssn')]\n",
    "    ssn_no = each_file[each_file.find('Ssn_')+4:each_file.find('.dats')]\n",
    "    \n",
    "    if len(sbj_id) < 2: sbj = \"sbj0\"+sbj_id\n",
    "    else: sbj = \"sbj\"+sbj_id\n",
    "    if len(ssn_no) < 2: ssn = \"ssn0\"+ssn_no\n",
    "    else: ssn = \"ssn\"+ssn_no\n",
    "\n",
    "    with open(input_path, 'rb') as handle:\n",
    "        rns_data = pickle.load(handle)\n",
    "\n",
    "    ## Add metadata to data\n",
    "\n",
    "    for key in rns_data.keys():\n",
    "        rns_data[key].append(return_metadata_from_name(key, metadata_jsons))\n",
    "        \n",
    "    motor_df = pd.DataFrame(rns_data['Unity_MotorInput'][0], columns=rns_data['Unity_MotorInput'][1],\n",
    "                      index=rns_data['Unity_MotorInput'][2]['ChannelNames']).T\n",
    "    motor_df = motor_df.reset_index().rename(columns={'index': 'timestamp'})\n",
    "    session_start_time = motor_df.timestamp.iloc[0]\n",
    "\n",
    "    motor_df.timestamp -= session_start_time\n",
    "\n",
    "    start_intervals, end_intervals, inactive_duration = find_breaks(motor_df['throttle_input'].tolist(), motor_df.timestamp.tolist(),break_time=5, inactive_value=0)\n",
    "    sub_df = pd.DataFrame({'sbj_ssn': sbj+ssn, 'start_intervals': start_intervals,'end_intervals': end_intervals, 'inactive_duration': inactive_duration})\n",
    "    all_dfs.append(sub_df)\n",
    "# pd.concat(all_dfs).to_csv('all_motor_break_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230044a9-7e72-4580-afd6-13c070530d4b",
   "metadata": {},
   "source": [
    "### Post-annotation, clean up the motor data and create simulated input for Unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fa10ac2-669b-460d-80f3-d88e576be5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df = pd.read_csv(f\"{data_dir}/annotated/all_raw_trial_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7bab0da0-236d-4d70-ae8d-22f81e83ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(f\"{data_dir}/annotated/all_motor_break_events.xlsx\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for sub in df.sbj_ssn.unique():\n",
    "# for sub in ['sbj21ssn03']:\n",
    "    start_intervals = []\n",
    "    simulate = False\n",
    "    sub_df = df.loc[df.sbj_ssn==sub].reset_index(drop=True)\n",
    "    sbj_ssn = sub_df.sbj_ssn.iloc[0]\n",
    "    if sub_df.regular_calibration_intervals.iloc[0]: simulate = True # only simulate if we confirmed that the pre-calibration intervals are good\n",
    "    if not simulate: continue\n",
    "    \n",
    "    # start_intervals = []\n",
    "    drive_type = []\n",
    "    start_intervals.append({'sbj_ssn': sbj_ssn, 'start_interval': sub_df.iloc[0].start_intervals, 'type_interval': 'brake', 'type_trial':'practice'})\n",
    "    \n",
    "    calib_trial_count = 5\n",
    "    calibration_interval_start = sub_df.loc[sub_df.calibration_trials==1,'start_intervals'] # confirmed first calibration interval \n",
    "    if len(calibration_interval_start) == 1:\n",
    "        calib_idx = sub_df.loc[sub_df.calibration_trials==1,'start_intervals'].index.values[0]\n",
    "        calib_df = sub_df.iloc[:calib_idx+1]\n",
    "        total_driving_time = np.nansum(calib_df.start_intervals.shift(-1)-calib_df.end_intervals)\n",
    "        for index, interval in calib_df.iloc[1:].iterrows():\n",
    "            average_trial_duration = total_driving_time/calib_trial_count\n",
    "            start_drive = calib_df.iloc[index-1].end_intervals\n",
    "            end_drive = interval.start_intervals\n",
    "            if index == calib_df.index[-1]: # if it's the last chance to get calibration trial, ensure we will get 5 trials\n",
    "                num_complete_trials = calib_trial_count\n",
    "            else:\n",
    "                num_complete_trials = math.floor((end_drive-start_drive)/average_trial_duration)\n",
    "            if num_complete_trials == 0:\n",
    "                d_time = end_drive-start_drive\n",
    "                total_driving_time -= d_time # we won't be able to complete a trial but make a little progress\n",
    "                start_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': start_drive+d_time, 'type_interval':'brake', 'type_trial':'practice'})\n",
    "            else:\n",
    "                start_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': start_drive, 'type_interval':'drive', 'type_trial':'practice'})\n",
    "            interval_done = start_drive\n",
    "            for trial in range(num_complete_trials):\n",
    "                trial_duration = random.uniform(average_trial_duration-.5,average_trial_duration+.5)\n",
    "                # trial_duration = average_trial_duration\n",
    "                total_driving_time -= trial_duration\n",
    "                interval_done = interval_done+trial_duration\n",
    "                if trial == num_complete_trials-1: # last one is a brake\n",
    "                    start_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': interval_done, 'type_interval':'brake', 'type_trial':'practice'})\n",
    "                else:\n",
    "                    start_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': interval_done, 'type_interval':'drive', 'type_trial':'practice'})\n",
    "                left_over = end_drive-interval_done\n",
    "                average_trial_duration = left_over/(num_complete_trials-trial-1) \n",
    "                calib_trial_count -= 1\n",
    "    \n",
    "    \n",
    "    sub = int(sbj_ssn.split('sbj')[1].split('ssn')[0])\n",
    "    ses = int(sbj_ssn.split('sbj')[1].split('ssn')[1])\n",
    "    def process_sub_df(calib_idx, trial_df, sub, ses, between_calib=False, calib_block_to_end=1):\n",
    "        # calib_block_to_end == 1 if wanting to get the number of trials after the calibration block  (assuming 2 is first calib block)\n",
    "        # calib_block_to_end == 11 if wanting to get the number of trials after the second calibraiton block (assuming 12 is second calib block)\n",
    "        # calib_idx should map onto when in sub_df the calibration corresponds to (motor breaks)\n",
    "        sub_df_intervals = []\n",
    "        if between_calib:\n",
    "            calib_df = sub_df.iloc[calib_idx:between_calib+1].reset_index(drop=True)\n",
    "            trials_between_calib = len(trial_df.loc[(trial_df.ppid==sub) & (trial_df.session==ses) & (trial_df.block>1) & (trial_df.block<=11)])\n",
    "        elif calib_block_to_end:\n",
    "            calib_df = sub_df.iloc[calib_idx:].reset_index(drop=True)\n",
    "            trials_between_calib = len(trial_df.loc[(trial_df.ppid==sub) & (trial_df.session==ses) & (trial_df.block>calib_block_to_end)])\n",
    "        calib_trial_count = trials_between_calib\n",
    "        total_driving_time = np.nansum(calib_df.start_intervals.shift(-1)-calib_df.end_intervals)\n",
    "        for index, interval in calib_df.iloc[1:].iterrows():\n",
    "            average_trial_duration = total_driving_time/calib_trial_count\n",
    "            start_drive = calib_df.iloc[index-1].end_intervals\n",
    "            end_drive = interval.start_intervals\n",
    "            # sub_df_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': start_drive, 'type_interval':'drive'})\n",
    "            num_complete_trials = math.floor((end_drive-start_drive)/average_trial_duration)\n",
    "            if num_complete_trials == 0:\n",
    "                d_time = end_drive-start_drive\n",
    "                total_driving_time -= d_time # we won't be able to complete a trial but make a little progress\n",
    "                sub_df_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': start_drive+d_time, 'type_interval':'brake', 'type_trial':'voice'})\n",
    "            else:\n",
    "                sub_df_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': start_drive, 'type_interval':'drive', 'type_trial':'voice'})\n",
    "            interval_done = start_drive\n",
    "            for trial in range(num_complete_trials):\n",
    "                trial_duration = random.uniform(average_trial_duration-.5,average_trial_duration+.5)\n",
    "                # trial_duration = average_trial_duration\n",
    "                total_driving_time -= trial_duration\n",
    "                interval_done = interval_done+trial_duration\n",
    "                if trial == num_complete_trials-1: # last one is a brake\n",
    "                    sub_df_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': interval_done, 'type_interval':'brake', 'type_trial':'voice'})\n",
    "                else:\n",
    "                    sub_df_intervals.append({'sbj_ssn': interval.sbj_ssn, 'start_interval': interval_done, 'type_interval':'drive', 'type_trial':'voice'})\n",
    "                left_over = end_drive-interval_done\n",
    "                average_trial_duration = left_over/(num_complete_trials-trial-1) \n",
    "                calib_trial_count -= 1\n",
    "        return sub_df_intervals, calib_df\n",
    "    \n",
    "    calibration_interval_start = sub_df.loc[sub_df.calibration_trials==2,'start_intervals'] # confirmed second calibration interval \n",
    "    if len(calibration_interval_start) == 1:\n",
    "        calib_idx_2 = sub_df.loc[sub_df.calibration_trials==2,'start_intervals'].index.values[0]\n",
    "        sub_df_intervals, calib_df = process_sub_df(calib_idx, trial_df, sub, ses, between_calib=calib_idx_2)\n",
    "        start_intervals.extend(sub_df_intervals)\n",
    "        # now take care of the rest\n",
    "        calib_idx = calib_idx_2\n",
    "        sub_df_intervals, calib_df = process_sub_df(calib_idx, trial_df, sub, ses, between_calib=False, calib_block_to_end=11)\n",
    "        \n",
    "        start_intervals.extend(sub_df_intervals)\n",
    "    else:\n",
    "        sub_df_intervals, calib_df = process_sub_df(calib_idx, trial_df, sub, ses, between_calib=False, calib_block_to_end=1)\n",
    "        start_intervals.extend(sub_df_intervals)\n",
    "    #if len(calibration_interval_start) == 1\n",
    "    #    calib_idx_2 = sub_df.index[-1]\n",
    "    #    sub_df_intervals = process_sub_df(calib_idx_2, trial_df, sub, ses)\n",
    "    #    start_intervals.extend(sub_df_intervals)\n",
    "    \n",
    "    out_df = pd.DataFrame(start_intervals)\n",
    "    out_df['end_interval'] = out_df.start_interval.shift(-1)\n",
    "    out_df.loc[out_df.index[-1],'end_interval'] = calib_df.end_intervals.iloc[-1]\n",
    "    out_df = out_df[['sbj_ssn', 'start_interval', 'end_interval', 'type_interval','type_trial']]\n",
    "    all_dfs.append(out_df)\n",
    "all_dfs = pd.concat(all_dfs)\n",
    "all_dfs.to_csv('motor_events_simulation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "mna",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "mna",
   "language": "python",
   "name": "mna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
