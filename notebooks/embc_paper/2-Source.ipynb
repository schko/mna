{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyvistaqt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath_to_base_package\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmne\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m mne\u001b[39m.\u001b[39;49mviz\u001b[39m.\u001b[39;49mset_3d_backend(\u001b[39m'\u001b[39;49m\u001b[39mpyvistaqt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[0;32m<decorator-gen-167>:12\u001b[0m, in \u001b[0;36mset_3d_backend\u001b[0;34m(backend_name, verbose)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/mna/lib/python3.10/site-packages/mne/viz/backends/renderer.py:130\u001b[0m, in \u001b[0;36mset_3d_backend\u001b[0;34m(backend_name, verbose)\u001b[0m\n\u001b[1;32m    128\u001b[0m backend_name \u001b[39m=\u001b[39m _check_3d_backend_name(backend_name)\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m MNE_3D_BACKEND \u001b[39m!=\u001b[39m backend_name:\n\u001b[0;32m--> 130\u001b[0m     _reload_backend(backend_name)\n\u001b[1;32m    131\u001b[0m     MNE_3D_BACKEND \u001b[39m=\u001b[39m backend_name\n\u001b[1;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m old_backend_name\n",
      "File \u001b[0;32m/opt/conda/envs/mna/lib/python3.10/site-packages/mne/viz/backends/renderer.py:32\u001b[0m, in \u001b[0;36m_reload_backend\u001b[0;34m(backend_name)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_reload_backend\u001b[39m(backend_name):\n\u001b[1;32m     31\u001b[0m     \u001b[39mglobal\u001b[39;00m backend\n\u001b[0;32m---> 32\u001b[0m     backend \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(name\u001b[39m=\u001b[39;49m_backend_name_map[backend_name],\n\u001b[1;32m     33\u001b[0m                                       package\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmne.viz.backends\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     34\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m 3d backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m backend_name)\n",
      "File \u001b[0;32m/opt/conda/envs/mna/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/mna/lib/python3.10/site-packages/mne/viz/backends/_qt.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mweakref\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyvista\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyvistaqt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplotting\u001b[39;00m \u001b[39mimport\u001b[39;00m FileDialog, MainWindow\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfigure\u001b[39;00m \u001b[39mimport\u001b[39;00m Figure\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_qt5agg\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvas\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyvistaqt'"
     ]
    }
   ],
   "source": [
    "path_to_base_package = '../..'\n",
    "import sys\n",
    "# setting path\n",
    "sys.path.append(f'{path_to_base_package}')\n",
    "import mne\n",
    "mne.viz.set_3d_backend('pyvistaqt')\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mna.utils.data_access import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'{path_to_base_package}/output/saved_files/pickle_files/corrected_voice_timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_dir}/ica_epochs.pickle', 'rb') as handle:\n",
    "    ica_proc_epochs = pickle.load(handle)\n",
    "    \n",
    "all_epochs = mne.concatenate_epochs(list(ica_proc_epochs.values()))\n",
    "\n",
    "# get EEG data and labels from concatenated epoch objects\n",
    "proc_epochs_data = all_epochs.get_data()\n",
    "proc_labels = all_epochs.events[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fsaverage files\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = os.path.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "subject = 'fsaverage'\n",
    "trans = 'fsaverage'  # MNE has a built-in fsaverage transformation\n",
    "'''\n",
    "select the boundary element model, note that the source data has been downsampled by a factor of 5 \n",
    "(i.e. ico == 5, https://mne.tools/stable/generated/mne.setup_source_space.html#mne.setup_source_space)\n",
    "and the BEM has been downsampled by a factor of 5 (i.e. ico == 4, see here: https://mne.tools/stable/generated/mne.make_bem_model.html)\n",
    "implications here: https://brainder.org/2016/05/31/downsampling-decimating-a-brain-surface/\n",
    "'''\n",
    "src_fname = os.path.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\n",
    "bem = os.path.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\n",
    "\n",
    "eeg_montage='biosemi64'\n",
    "info = all_epochs.info\n",
    "\n",
    "# Read and set the EEG electrode locations, which are already in fsaverage's\n",
    "# space (MNI space) for standard_1020:\n",
    "montage = mne.channels.make_standard_montage(eeg_montage)\n",
    "\n",
    "# Check that the locations of EEG electrodes is correct with respect to MRI\n",
    "#mne.viz.plot_alignment(\n",
    "#    info, src=src_fname, eeg=['original', 'projected'], trans=trans,\n",
    "#    show_axes=False, mri_fiducials=True, dig='fiducials')\n",
    "fwd = mne.make_forward_solution(info, trans=trans, src=src_fname,\n",
    "                                bem=bem, eeg=True, n_jobs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"eLORETA\"\n",
    "snr = 3.\n",
    "lambda2 = 1. / snr ** 2\n",
    "cov = mne.compute_covariance(all_epochs, method='auto') # note this is not average referenced\n",
    "cov.plot(all_epochs.info)\n",
    "inverse_operator = mne.minimum_norm.make_inverse_operator(info, fwd, cov)\n",
    "#low_motor = get_forward_results(output_dir, 'low',low_motor_sensor,inverse_operator,fwd, lambda2)\n",
    "#high_motor = get_forward_results(output_dir, 'high',high_motor_sensor,inverse_operator,fwd, lambda2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tcs(output_dir, overwrite=False):\n",
    "    if not overwrite and os.path.isfile(f\"{output_dir}source_time_courses.pickle\"):\n",
    "        all_tcs = pickle.load(open(f\"{output_dir}source_time_courses.pickle\", 'rb'))\n",
    "        return np.concatenate(all_tcs)\n",
    "    else:\n",
    "        bin_size = 500 # number of sources to compute in batch\n",
    "        mode = 'mean'\n",
    "        count = 0\n",
    "        all_tcs=[]\n",
    "        while count*bin_size <= len(all_epochs):\n",
    "            print('bin',count)\n",
    "            stcs = mne.minimum_norm.apply_inverse_epochs(all_epochs[count*bin_size:(count+1)*bin_size], inverse_operator,\n",
    "                                        lambda2=1.0 / snr ** 2, verbose=False,\n",
    "                                        method=\"eLORETA\") # , pick_ori=\"normal\"?\n",
    "            tcs = mne.extract_label_time_course(stcs, rel_labels, src=fwd['src'], mode=mode,verbose=False)\n",
    "            all_tcs.append(tcs)\n",
    "            count += 1\n",
    "        # plot the last time course\n",
    "        plot_source_time_course(ltc=tcs[0], orig_stc=stcs[0], label=rel_labels[0], mode = 'mean',rel_mappings=rel_mappings)\n",
    "        with open(f\"{output_dir}source_time_courses.pickle\", 'wb') as handle_tcs:\n",
    "            pickle.dump(all_tcs, handle_tcs, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return np.concatenate(all_tcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tcs = get_all_tcs(output_dir_non_baseline_non_average,overwrite=False)\n",
    "assert len(all_tcs) == len(all_epochs), 'the source data is not the same size as the motor_epochs, check this issue'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeg_features(df, data_type = 'processed', features = 'all', label_source = 'Steering_Wheel_Degree_Encoded', \n",
    "                 cleaned_up = False):\n",
    "    \n",
    "    if data_type == 'processed':\n",
    "        first_electrode_column_name = \"Fp1_4-8_Hz_Power\"\n",
    "        last_electrode_column_name = \"O2_32-55_Hz_Sample_entropy\"\n",
    "        autoreject_column_name = \"autorejected\"\n",
    "    elif data_type == 'raw':\n",
    "        first_electrode_column_name = \"Fp1_4-8_Hz_Power_raw\"\n",
    "        last_electrode_column_name = \"O2_32-55_Hz_Sample_entropy_raw\"\n",
    "        autoreject_column_name = \"autorejected_raw\"\n",
    "        \n",
    "    first_electrode_idx = df.columns.get_loc(first_electrode_column_name)\n",
    "    last_electrode_idx = df.columns.get_loc(last_electrode_column_name)\n",
    "\n",
    "    # with autoreject\n",
    "    valid_trial = (df[label_source].notnull()) & (df[autoreject_column_name] == False)\n",
    "    \n",
    "    all_eeg_features = df.iloc[:,first_electrode_idx:last_electrode_idx+1] # all features in cleaned up data\n",
    "    \n",
    "    if features == 'all': \n",
    "        eeg_features = all_eeg_features\n",
    "    else:\n",
    "        features = \"|\".join(map(str,features))\n",
    "        eeg_features = all_eeg_features.loc[:, all_eeg_features.columns.str.contains(features)]\n",
    "    \n",
    "    if cleaned_up:\n",
    "        return np.asarray(eeg_features[valid_trial]), np.asarray(df[label_source][valid_trial])\n",
    "    else:\n",
    "        return eeg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_features(df, features = \"pupil\", label_source = 'Steering_Wheel_Degree_Encoded', cleaned_up = False):\n",
    "    pupil_diameter = ['Left Pupil Diameter','Right Pupil Diameter']\n",
    "    \n",
    "    if features == 'pupil':\n",
    "        # pupil_diameter.append(label_source)\n",
    "        eye_df = df[pupil_diameter]\n",
    "    else:\n",
    "        eye_feature = features\n",
    "        # eye_feature.append(label_source)\n",
    "        eye_df = df[eye_feature]\n",
    "        \n",
    "    if cleaned_up:\n",
    "        eye_df = eye_df.join(df[label_source]).dropna()\n",
    "        return np.asarray(eye_df.iloc[:,0:-1]), np.asarray(eye_df.iloc[:,-1])\n",
    "    else:\n",
    "        return eye_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecg_features(df, features = \"all\", label_source = 'Steering_Wheel_Degree_Encoded', cleaned_up = False):\n",
    "    ecg_feature_first = df.columns.get_loc(\"bpm\")\n",
    "    ecg_feature_last = df.columns.get_loc(\"breathingrate\")\n",
    "    \n",
    "    if features == 'all':\n",
    "        ecg_df = df.iloc[:,ecg_feature_first:ecg_feature_last-2]\n",
    "    else:\n",
    "        ecg_feature = features\n",
    "        # ecg_feature.append(label_source)\n",
    "        ecg_df = df[ecg_feature]\n",
    "    \n",
    "    if cleaned_up:\n",
    "        ecg_df = ecg_df.join(df[label_source]).dropna()\n",
    "        return np.asarray(ecg_df.iloc[:,0:-1]), np.asarray(ecg_df.iloc[:,-1])\n",
    "    else:\n",
    "        return ecg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_features(df, label_source = 'Steering_Wheel_Degree_Encoded'):\n",
    "    \n",
    "    all_features_list = [eeg_features(df), eye_features(df), ecg_features(df), df[label_source]]\n",
    "    all_features_df = pd.concat(all_features_list, axis = 1).dropna()\n",
    "    \n",
    "    return np.asarray(all_features_df.iloc[:,0:-1]), np.asarray(all_features_df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Features Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_features(x_train, x_test):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train_norm = scaler.transform(x_train)\n",
    "    x_test_norm = scaler.transform(x_test)\n",
    "    \n",
    "    return x_train_norm, x_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EEG Features normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalization(x_data, y_label, train_percentage=0.8):\n",
    "    \n",
    "    # Remove rows with invalid pupil diameter\n",
    "    if sum(sum(np.isnan(x_data))) > 0:\n",
    "        invalid_trial = np.argwhere(np.any(np.isnan(x_data) == True, axis=1))\n",
    "        x_data_corrected = np.delete(x_data, invalid_trial, axis=0)\n",
    "        y_label_corrected = np.delete(y_label, invalid_trial, axis=0)\n",
    "\n",
    "    else:\n",
    "        x_data_corrected = x_data\n",
    "        y_label_corrected = y_label\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data_corrected, y_label_corrected, \n",
    "                                                                            train_size = train_percentage, random_state=rs)\n",
    "    \n",
    "    norm_data = MinMaxScaler().fit(x_train)\n",
    "    x_train_norm = norm_data.transform(x_train)\n",
    "    x_test_norm = norm_data.transform(x_test)\n",
    "\n",
    "    return x_train_norm, x_test_norm, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_classification(train_data, test_data, train_label, test_label, classfier,\n",
    "                         data_type = None, save_plots = True, plot_fig = True):\n",
    "\n",
    "    if classfier == 'logistic':\n",
    "\n",
    "        logreg = LogisticRegression(solver=\"liblinear\", penalty = 'l1', random_state=0).fit(train_data, train_label)\n",
    "        score_train = logreg.decision_function(train_data)\n",
    "        score_test = logreg.decision_function(test_data)\n",
    "\n",
    "        train_pred = logreg.predict(train_data)\n",
    "        test_pred = logreg.predict(test_data)\n",
    "        \n",
    "        importance = logreg.coef_[0]\n",
    "\n",
    "    if classfier == 'svm':\n",
    "\n",
    "        svm_classifer = SGDClassifier(penalty = 'l1',l1_ratio=1, random_state=0).fit(train_data, train_label)\n",
    "        score_train = svm_classifer.decision_function(train_data)\n",
    "        score_test = svm_classifer.decision_function(test_data)\n",
    "\n",
    "        train_pred = svm_classifer.predict(train_data)\n",
    "        test_pred = svm_classifer.predict(test_data)\n",
    "        \n",
    "        importance = svm_classifer.coef_[0]\n",
    "\n",
    "    if classfier == 'knn':\n",
    "        kNN = KNeighborsClassifier(n_neighbors = 3).fit(train_data, train_label)\n",
    "\n",
    "        score_train = kNN.predict_proba(train_data)[:,1]\n",
    "        score_test = kNN.predict_proba(test_data)[:,1]\n",
    "\n",
    "        train_pred = (score_train > 0.5) + 0\n",
    "        test_pred = (score_test >0.5) + 0\n",
    "        \n",
    "    if classfier == 'random_forest':\n",
    "        random_forest = RandomForestClassifier(n_estimators=100, max_features = 'sqrt', random_state = rs, \n",
    "                                               bootstrap=True, class_weight = 'balanced_subsample', \n",
    "                                               max_samples=None, n_jobs = -1).fit(train_data, train_label)\n",
    "        \n",
    "        score_train = random_forest.predict_proba(train_data)[:,1]\n",
    "        score_test = random_forest.predict_proba(test_data)[:,1]\n",
    "        \n",
    "        train_pred = random_forest.predict(train_data)\n",
    "        test_pred = random_forest.predict(test_data)\n",
    "        \n",
    "        importance = random_forest.feature_importances_\n",
    "\n",
    "    fpr_train, tpr_train, thresholds_train = metrics.roc_curve(train_label-1, score_train)\n",
    "    AUC_train = metrics.roc_auc_score (train_label-1, score_train)\n",
    "\n",
    "    fpr_test, tpr_test, thresholds_test = metrics.roc_curve(test_label-1, score_test)\n",
    "    AUC_test = metrics.roc_auc_score (test_label-1, score_test)\n",
    "\n",
    "    train_acc = metrics.accuracy_score(train_label,train_pred)\n",
    "    test_acc = metrics.accuracy_score(test_label,test_pred)\n",
    "    \n",
    "    if plot_fig:\n",
    "        # ROC Curve\n",
    "        sns.set(font_scale=2)\n",
    "        plt.style.use('seaborn-white')\n",
    "        fig = plt.figure(figsize = [25,7])\n",
    "\n",
    "        axe = fig.add_subplot(1,2,1)\n",
    "        axe.plot(fpr_train,tpr_train)\n",
    "        axe.set_xlabel(\"False Positive Rate\")\n",
    "        axe.set_ylabel(\"True Positive Rate\")\n",
    "        axe.set_title(\"Training ROC Curve\")\n",
    "        axe.text(0.6,0.2,\"AUC = {:.2f}\".format(AUC_train))\n",
    "\n",
    "        axe = fig.add_subplot(1,2,2)\n",
    "        axe.plot(fpr_test,tpr_test)\n",
    "        axe.set_xlabel(\"False Positive Rate\")\n",
    "        axe.set_ylabel(\"True Positive Rate\")\n",
    "        axe.set_title(\"Testing ROC Curve\")\n",
    "        axe.text(0.6,0.2,\"AUC = {:.2f}\".format(AUC_test))\n",
    "\n",
    "        if save_plots:\n",
    "            plt.savefig(f\"../output/classification_result/{data_type}_data_Training_Testing_ROC_Curve.png\", dpi=300)\n",
    "\n",
    "        # plt.grid(visible=False)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        # fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "        fig_cnf = plt.figure(figsize = [20, 5])\n",
    "        ax1 = fig_cnf.add_subplot(1,2,1)\n",
    "        ax2 = fig_cnf.add_subplot(1,2,2)\n",
    "\n",
    "        cnf_matrix_train = metrics.confusion_matrix(train_label, train_pred)\n",
    "        cnf_matrix_test = metrics.confusion_matrix(test_label, test_pred)\n",
    "\n",
    "        sns.heatmap(cnf_matrix_train, fmt = 'g', annot = True, xticklabels = ['Easy','Hard'], yticklabels = ['Easy','Hard'],ax=ax1)\n",
    "        ax1.set_title(\"Training Confusion Matrix\")\n",
    "        sns.heatmap(cnf_matrix_test, fmt = 'g', annot = True, xticklabels = ['Easy','Hard'], yticklabels = ['Easy','Hard'],ax=ax2)\n",
    "        ax2.set_title(\"Testing Confusion Matrix\")\n",
    "\n",
    "        if save_plots:\n",
    "            plt.savefig(f\"../output/classification_result/{data_type}_data_Training_Testing_Confusion_Matrix.png\", dpi=300)\n",
    "\n",
    "        # Features Importance\n",
    "        fig_importance = plt.figure(figsize = [10 ,3])\n",
    "        axe = fig_importance.add_subplot(1,1,1)\n",
    "\n",
    "        markerline, stemline, baseline = axe.stem([x for x in range(len(importance))], importance, \n",
    "                                                  linefmt='k-',markerfmt='ko',basefmt='k.')\n",
    "        plt.setp(stemline, linewidth = 1)\n",
    "        plt.setp(markerline, markersize = 1)\n",
    "        axe.set_xlabel(\"Feature\")\n",
    "        axe.set_ylabel(\"Importance\")\n",
    "        axe.set_title(\"Coefficient for Each Features\")\n",
    "        if save_plots:\n",
    "            plt.savefig(f\"../output/classification_result/{data_type}_data_coefficient.png\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return AUC_train, AUC_test, importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modality_cv(x_modality, y_modality, n_folds = 10, classifier = 'logistic'):\n",
    "    \n",
    "    auc_list = np.empty((2, n_folds))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = n_folds, random_state=rs, shuffle=True)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(x_modality, y_modality)):\n",
    "\n",
    "        x_train_norm, x_test_norm = norm_features(x_modality[train_index], x_modality[test_index])\n",
    "        train_auc, test_auc, coefs = trial_classification(x_train_norm, x_test_norm,\n",
    "                                                          y_modality[train_index], y_modality[test_index],\n",
    "                                                          classifier, plot_fig = False)\n",
    "        auc_list[0,i] = train_auc\n",
    "        auc_list[1,i] = test_auc\n",
    "    \n",
    "    return np.mean(auc_list, axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Root Mean Square Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(df, modality, true_val_col = 'Steering_Wheel_Degree', features_list = 'all'):\n",
    "    \n",
    "    if modality == \"EEG\":\n",
    "        x_modality, y_modality = eeg_features(df, features = features_list, label_source = true_val_col, \n",
    "                                              cleaned_up = True)\n",
    "    if modality == \"Eye\":\n",
    "        if features_list == 'all':\n",
    "            features_list = [\"Left Pupil Diameter\", \"Right Pupil Diameter\",\n",
    "                            \"Left Evoked Pupil Diameter\", \"Right Evoked Pupil Diameter\"]\n",
    "        x_modality, y_modality = eye_features(df, features = features_list, label_source = true_val_col, \n",
    "                                              cleaned_up = True)\n",
    "    if modality == \"ECG\":\n",
    "        x_modality, y_modality = ecg_features(df, features = features_list, label_source = true_val_col, \n",
    "                                              cleaned_up = True)\n",
    "    if modality == \"All\":\n",
    "        x_modality, y_modality = multimodal_features(df, label_source = true_val_col)\n",
    "        \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_modality, y_modality, test_size=0.3, random_state=rs)\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = regr.predict(X_test)\n",
    "    modality_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    return y_test, y_pred, modality_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process and Save All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_dfs = None\n",
    "\n",
    "# ica_epochs_dict = {}\n",
    "# ica_dict = {}\n",
    "# eog_idx_dict = {}\n",
    "# events_dict = {}\n",
    "\n",
    "# for each_file in onlyfiles:\n",
    "#     input_path = data_dir + each_file\n",
    "    \n",
    "#     sbj_id = each_file[each_file.find('Sbj_')+4:each_file.find('-Ssn')]\n",
    "#     ssn_no = each_file[each_file.find('Ssn_')+4:each_file.find('.dats')]\n",
    "\n",
    "#     if len(sbj_id) < 2: sbj = \"sbj0\"+sbj_id\n",
    "#     else: sbj = \"sbj\"+sbj_id\n",
    "#     if len(ssn_no) < 2: ssn = \"ssn0\"+ssn_no\n",
    "#     else: ssn = \"ssn\"+ssn_no\n",
    "    \n",
    "#     if sbj+ssn == \"sbj20ssn03\":\n",
    "#         ref_ica = None\n",
    "#     else: \n",
    "#         ref_ica = ica_dict['sbj20ssn03']\n",
    "    \n",
    "#     with open(input_path, 'rb') as handle:\n",
    "#         rns_data = pickle.load(handle)\n",
    "\n",
    "#     ## Add metadata to data\n",
    "\n",
    "#     for key in rns_data.keys():\n",
    "#         rns_data[key].append(return_metadata_from_name(key, metadata_jsons))\n",
    "\n",
    "#     event_df = event_data_from_data(rns_data, interrupted_id_sessions=[(13,1), (22,1)])\n",
    "#     event_df['trial_damage'] = event_df.damage.diff().fillna(0)\n",
    "#     event_df['trial_duration'] = event_df.trial_end_time - event_df.trial_start_time\n",
    "\n",
    "#     percent_missing = event_df.notnull().sum() / len(event_df)\n",
    "#     summary_statistics = {}\n",
    "#     summary_statistics['voice_success_rate'] = percent_missing['voice_timestamp']\n",
    "#     if 'chunk_timestamp' in percent_missing:\n",
    "#         summary_statistics['chunk_success_rate'] = percent_missing['chunk_timestamp']\n",
    "#     else:\n",
    "#         summary_statistics['chunk_success_rate'] = 0\n",
    "\n",
    "#     # temporary fix for pilot phase where we had some incomplete data\n",
    "#     if 'block_condition' not in event_df:\n",
    "#         event_df['block_condition'] = 'practice'\n",
    "#         event_df.loc[5:,'block_condition'] = 'voice'\n",
    "\n",
    "#     event_df['spoken_difficulty_encoded'] = event_df.spoken_difficulty.replace(to_replace=['easy', 'hard', 'unknown'],\n",
    "#                                                                           value=[1, 2, None])\n",
    "\n",
    "#     # ecg\n",
    "#     post_processed_event_df = process_session_ecg(rns_data, event_df,plot_frequency=20,plot_ecg_snippet=40)\n",
    "\n",
    "#     # eye\n",
    "#     # post_processed_event_df = process_session_eye(rns_data, post_processed_event_df,detect_blink=True,pretrial_period=0,\n",
    "#     #                                               posttrial_period=0,plot_frequency=20, plot_eye_snippet=40, classifiers=['NSLR'])\n",
    "#     if 'Unity_ViveSREyeTracking' in rns_data:\n",
    "#         post_processed_event_df = process_session_eye(rns_data, post_processed_event_df,detect_blink=True,\n",
    "#                                                       pretrial_period=0, posttrial_period=0, plot_frequency=20, \n",
    "#                                                       plot_eye_snippet=40, classifiers=['NSLR'])\n",
    "\n",
    "#     # eeg\n",
    "#     post_processed_event_df, epochs, events, info, reject_log, ica, eog_idx= process_session_eeg(rns_data, post_processed_event_df,\n",
    "#                                 run_autoreject=True, run_ica=True, save_raw_eeg = True, sbj_session = sbj+ssn, \n",
    "#                                 template_ica = ref_ica, analyze_pre_ica = True)\n",
    "    \n",
    "#     # motor\n",
    "#     post_processed_event_df, turns_df = process_session_motor(rns_data, post_processed_event_df, motor_channel='Unity_MotorInput',\n",
    "#                                                 plot_motor_result = True, plot_motor_snippet = 30, plot_frequency = 10)\n",
    "#     if motor_events:\n",
    "#         post_processed_event_df = turns_df\n",
    "    \n",
    "#     events_dict[sbj+ssn] = events\n",
    "#     ica_epochs_dict[sbj+ssn] = epochs\n",
    "#     ica_dict[sbj+ssn] = ica\n",
    "#     eog_idx_dict[sbj+ssn] = eog_idx\n",
    "    \n",
    "#     # save data for later use\n",
    "#     if save_data_pkl:\n",
    "        \n",
    "#         with open('../output/saved_files/pickle_files/all_events.pickle', 'wb') as handle_events:\n",
    "#             pickle.dump(events_dict, handle_events, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         with open('../output/saved_files/pickle_files/ica_epochs.pickle', 'wb') as handle_ica_eps:\n",
    "#             pickle.dump(ica_epochs_dict, handle_ica_eps, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         with open('../output/saved_files/pickle_files/ica.pickle', 'wb') as handle_ica:\n",
    "#             pickle.dump(ica_dict, handle_ica, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         with open('../output/saved_files/pickle_files/eog_comp.pickle', 'wb') as handle_eog:\n",
    "#             pickle.dump(eog_idx_dict, handle_eog, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#     # save\n",
    "#     post_processed_event_df.to_csv(f\"{output_dir}ppid_{post_processed_event_df.iloc[0].ppid}_session_{post_processed_event_df.iloc[0].session}.csv\")\n",
    "#     if not type(all_dfs)==pd.core.frame.DataFrame:\n",
    "#         all_dfs = post_processed_event_df\n",
    "#     else:\n",
    "#         all_dfs = pd.concat([all_dfs, post_processed_event_df], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_dfs = None\n",
    "\n",
    "ica_epochs_dict = {}\n",
    "ica_dict = {}\n",
    "eog_idx_dict = {}\n",
    "events_dict = {}\n",
    "\n",
    "for each_file in onlyfiles:\n",
    "    input_path = data_dir + each_file\n",
    "    \n",
    "    sbj_id = each_file[each_file.find('Sbj_')+4:each_file.find('-Ssn')]\n",
    "    ssn_no = each_file[each_file.find('Ssn_')+4:each_file.find('.dats')]\n",
    "\n",
    "    if len(sbj_id) < 2: sbj = \"sbj0\"+sbj_id\n",
    "    else: sbj = \"sbj\"+sbj_id\n",
    "    if len(ssn_no) < 2: ssn = \"ssn0\"+ssn_no\n",
    "    else: ssn = \"ssn\"+ssn_no\n",
    "    \n",
    "    if sbj+ssn == \"sbj20ssn03\":\n",
    "        ref_ica = None\n",
    "    else: \n",
    "        ref_ica = ica_dict['sbj20ssn03']\n",
    "    \n",
    "    with open(input_path, 'rb') as handle:\n",
    "        rns_data = pickle.load(handle)\n",
    "\n",
    "    ## Add metadata to data\n",
    "\n",
    "    for key in rns_data.keys():\n",
    "        rns_data[key].append(return_metadata_from_name(key, metadata_jsons))\n",
    "\n",
    "    event_df = read_event_data(rns_data) # typically only 15_1 and 22_1 will be used here, change below too\n",
    "\n",
    "    if event_df.empty:\n",
    "        continue\n",
    "    \n",
    "    event_df = event_df[event_df.block_condition == 'voice']\n",
    "    event_df['trial_damage'] = event_df.damage.diff().fillna(0)\n",
    "    event_df['trial_duration'] = event_df.trial_end_time - event_df.trial_start_time\n",
    "\n",
    "    percent_missing = event_df.notnull().sum() / len(event_df)\n",
    "    summary_statistics = {}\n",
    "    summary_statistics['voice_success_rate'] = percent_missing['spoken_difficulty']\n",
    "    event_df['spoken_difficulty'] = event_df['spoken_difficulty'].fillna(\"unknown\")\n",
    "    event_df['spoken_difficulty_encoded'] = event_df.spoken_difficulty.replace(to_replace=['easy', 'hard', 'unknown'],\n",
    "                                                                          value=[1, 2, 0])\n",
    "    \n",
    "#     event_df = event_data_from_data(rns_data, interrupted_id_sessions=[(13,1), (22,1)])\n",
    "#     event_df = event_data_from_data(rns_data, ts_fixer, remove_id_sessions=remove_sessions, interrupted_id_sessions=interrupted_sessions)\n",
    "#     if event_df.empty:\n",
    "#         continue\n",
    "    \n",
    "#     event_df['trial_damage'] = event_df.damage.diff().fillna(0)\n",
    "#     event_df['trial_duration'] = event_df.trial_end_time - event_df.trial_start_time\n",
    "\n",
    "#     percent_missing = event_df.notnull().sum() / len(event_df)\n",
    "#     summary_statistics = {}\n",
    "#     summary_statistics['voice_success_rate'] = percent_missing['voice_timestamp']\n",
    "#     if 'chunk_timestamp' in percent_missing:\n",
    "#         summary_statistics['chunk_success_rate'] = percent_missing['chunk_timestamp']\n",
    "#     else:\n",
    "#         summary_statistics['chunk_success_rate'] = 0\n",
    "\n",
    "#     # temporary fix for pilot phase where we had some incomplete data\n",
    "#     if 'block_condition' not in event_df:\n",
    "#         event_df['block_condition'] = 'practice'\n",
    "#         event_df.loc[5:,'block_condition'] = 'voice'\n",
    "\n",
    "#     event_df['spoken_difficulty_encoded'] = event_df.spoken_difficulty.replace(to_replace=['easy', 'hard', 'unknown'],\n",
    "#                                                                           value=[1, 2, None])\n",
    "\n",
    "    # ecg\n",
    "    post_processed_event_df = process_session_ecg(rns_data, event_df,plot_frequency=20,plot_ecg_snippet=40)\n",
    "\n",
    "    # eye\n",
    "    # post_processed_event_df = process_session_eye(rns_data, post_processed_event_df,detect_blink=True,pretrial_period=0,\n",
    "    #                                               posttrial_period=0,plot_frequency=20, plot_eye_snippet=40, classifiers=['NSLR'])\n",
    "    if 'Unity_ViveSREyeTracking' in rns_data:\n",
    "        post_processed_event_df = process_session_eye(rns_data, post_processed_event_df,detect_blink=True,\n",
    "                                                      pretrial_period=0, posttrial_period=0, plot_frequency=20, \n",
    "                                                      plot_eye_snippet=40, classifiers=['NSLR'])\n",
    "\n",
    "    # eeg\n",
    "    post_processed_event_df, epochs, events, info, reject_log, ica, eog_idx= process_session_eeg(rns_data, post_processed_event_df,\n",
    "                                run_autoreject=True, run_ica=True, save_raw_eeg = True, sbj_session = sbj+ssn, \n",
    "                                template_ica = ref_ica, analyze_pre_ica = True)\n",
    "    \n",
    "    # motor\n",
    "    post_processed_event_df, turns_df = process_session_motor(rns_data, post_processed_event_df, motor_channel='Unity_MotorInput',\n",
    "                                                plot_motor_result = True, plot_motor_snippet = 30, plot_frequency = 10)\n",
    "    if motor_events:\n",
    "        post_processed_event_df = turns_df\n",
    "    \n",
    "    events_dict[sbj+ssn] = events\n",
    "    ica_epochs_dict[sbj+ssn] = epochs\n",
    "    ica_dict[sbj+ssn] = ica\n",
    "    eog_idx_dict[sbj+ssn] = eog_idx\n",
    "    \n",
    "    # save data for later use\n",
    "    if save_data_pkl:\n",
    "        \n",
    "        with open('../output/saved_files/corrected_timestamp/all_events.pickle', 'wb') as handle_events:\n",
    "            pickle.dump(events_dict, handle_events, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('../output/saved_files/corrected_timestamp/ica_epochs.pickle', 'wb') as handle_ica_eps:\n",
    "            pickle.dump(ica_epochs_dict, handle_ica_eps, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('../output/saved_files/corrected_timestamp/ica.pickle', 'wb') as handle_ica:\n",
    "            pickle.dump(ica_dict, handle_ica, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('../output/saved_files/corrected_timestamp/eog_comp.pickle', 'wb') as handle_eog:\n",
    "            pickle.dump(eog_idx_dict, handle_eog, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # save\n",
    "    post_processed_event_df.to_csv(f\"{output_dir}ppid_{post_processed_event_df.iloc[0].ppid}_session_{post_processed_event_df.iloc[0].session}.csv\")\n",
    "    if not type(all_dfs)==pd.core.frame.DataFrame:\n",
    "        all_dfs = post_processed_event_df\n",
    "    else:\n",
    "        all_dfs = pd.concat([all_dfs, post_processed_event_df], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pivottablejs import pivot_ui\n",
    "all_dfs.to_csv(f\"../output/saved_files/corrected_voice_timestamp/all_results.csv\")\n",
    "all_dfs.to_excel(f\"../output/saved_files/corrected_voice_timestamp/all_results.xlsx\")\n",
    "# all_dfs.to_excel(f\"{output_dir}all_results.xlsx\")\n",
    "# pivot_ui(all_dfs, outfile_path=f\"{output_dir}all_results.html\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Epoching Raw EEG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load raw eeg.fif file and epoch raw eeg\n",
    "\n",
    "if epoch_raw_eeg:\n",
    "\n",
    "    with open('../output/saved_files/corrected_timestamp/all_events.pickle', 'rb') as handle:\n",
    "        all_events = pickle.load(handle)\n",
    "\n",
    "    raw_eeg_dir = '../output/saved_files/raw_eeg/'\n",
    "    event_dict = dict(easy=1, hard=2)\n",
    "\n",
    "    raw_eeg_dict = {}\n",
    "    raw_epochs_dict = {}\n",
    "\n",
    "    for sbj_ssn in list(all_events.keys()):\n",
    "\n",
    "        each_raw_eeg = sbj_ssn + '_eeg_filt_raw.fif'\n",
    "        raw_eeg_path = raw_eeg_dir+each_raw_eeg\n",
    "        raw_eeg = mne.io.read_raw_fif(raw_eeg_path, preload=True)\n",
    "        raw_eeg_dict[sbj_ssn] = raw_eeg\n",
    "\n",
    "        epochs_raw = mne.Epochs(raw_eeg, all_events[sbj_ssn], event_id=event_dict, baseline = (None, 0), tmin= -.2, tmax=3, preload=True, on_missing='warn')\n",
    "\n",
    "        autoreject_epochs = 20\n",
    "        run_autoreject = True\n",
    "\n",
    "        if len(epochs_raw) < 10: # we need at least 10 epochs to run autoreject for cross validation\n",
    "            # bad_epochs_raw = pd.Series(np.full(len(event_df),np.NAN), index=event_df.index, name='autorejected')\n",
    "            # event_df = event_df.join(bad_epochs)\n",
    "            reject_log = None\n",
    "        elif run_autoreject:\n",
    "            ar_raw = autoreject.AutoReject(random_state=rs,n_jobs=1, verbose=False)\n",
    "            ar_raw.fit(epochs_raw[:autoreject_epochs])  # fit on a few epochs to save time\n",
    "            epochs_ar, reject_log = ar_raw.transform(epochs_raw, return_log=True)\n",
    "            # bad_epochs = pd.Series(reject_log.bad_epochs, index=event_recognized_df.index, dtype=bool, name='autorejected')\n",
    "            # event_df = event_df.join(bad_epochs_raw) # creates nan if not processed at all\n",
    "            epochs_raw = epochs_ar\n",
    "\n",
    "        raw_epochs_dict[sbj_ssn] = epochs_raw\n",
    "\n",
    "    with open('../output/saved_files/corrected_voice_timestamp/raw_epochs.pickle', 'wb') as handle_raw_eps:\n",
    "        pickle.dump(raw_epochs_dict, handle_raw_eps, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('../output/saved_files/corrected_voice_timestamp/raw_eeg.pickle', 'wb') as handle_raw_eeg:\n",
    "        pickle.dump(raw_eeg_dict, handle_raw_eeg, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load All Processed Data and Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_dfs = pd.read_csv(\"../output/saved_files/corrected_voice_timestamp/all_results.csv\")\n",
    "\n",
    "# open saved pickle files\n",
    "with open('../output/saved_files/corrected_voice_timestamp/ica_epochs.pickle', 'rb') as handle:\n",
    "    all_proc_epochs = pickle.load(handle)\n",
    "with open('../output/saved_files/corrected_voice_timestamp/ica.pickle', 'rb') as handle:\n",
    "    all_ica = pickle.load(handle)\n",
    "with open('../output/saved_files/corrected_voice_timestamp/eog_comp.pickle', 'rb') as handle:\n",
    "    all_eog_comps = pickle.load(handle)\n",
    "with open('../output/saved_files/corrected_voice_timestamp/raw_epochs.pickle', 'rb') as handle:\n",
    "    all_raw_epochs = pickle.load(handle)\n",
    "\n",
    "# save ICA components plot\n",
    "if save_ica_plts:\n",
    "    ica_comp_dir = \"../output/plots/ica_comps/\"\n",
    "    if not os.path.isdir(ica_comp_dir): os.makedirs(ica_comp_dir)\n",
    "\n",
    "    for sbj_ssn in list(all_ica.keys()):\n",
    "        \n",
    "        all_ica[sbj_ssn].plot_components(picks = list(range(0,20)), title=sbj_ssn+\"_ICA_Components\", show=False)\n",
    "\n",
    "        plt.savefig(f\"{ica_comp_dir}{sbj_ssn}_ica_comps.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed component for all sessions\n",
    "\n",
    "show_removed_comp = False\n",
    "\n",
    "if show_removed_comp:\n",
    "    for sbj in all_ica.keys():\n",
    "        if all_eog_comps[sbj] != []:\n",
    "\n",
    "            all_ica[sbj].plot_components(picks = all_eog_comps[sbj], title=sbj, show=False)\n",
    "\n",
    "            plt.savefig(f\"../output/plots/Removed_Components_Corrmap/{sbj}_removed_components.png\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "# all_eog_comps.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all epochs\n",
    "raw_epochs_concat = mne.concatenate_epochs(list(all_raw_epochs.values()))\n",
    "proc_epochs_concat = mne.concatenate_epochs(list(all_proc_epochs.values()))\n",
    "\n",
    "# get EEG data and labels from concatenated epoch objects\n",
    "raw_epochs_eeg = raw_epochs_concat.get_data()\n",
    "proc_epochs_eeg = proc_epochs_concat.get_data()\n",
    "\n",
    "raw_labels = raw_epochs_concat.events[:,2]\n",
    "proc_labels = proc_epochs_concat.events[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_epochs_concat['easy'].plot(n_epochs=5, n_channels=3)\n",
    "# raw_epochs_concat['hard'].plot(n_epochs=5, n_channels=3)\n",
    "# proc_epochs_concat['easy'].plot(n_epochs=5, n_channels=3)\n",
    "# proc_epochs_concat['hard'].plot(n_epochs=5, n_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_raw_epoch = dict(sorted(all_raw_epochs.items()))\n",
    "# sorted_proc_epoch = dict(sorted(all_proc_epochs.items()))\n",
    "\n",
    "# raw_epochs_train = mne.concatenate_epochs(list(sorted_raw_epoch.values())[:-4]).get_data()\n",
    "# raw_epochs_test = mne.concatenate_epochs(list(sorted_raw_epoch.values())[-4:]).get_data()\n",
    "# proc_epochs_train = mne.concatenate_epochs(list(sorted_proc_epoch.values())[:-4]).get_data()\n",
    "# proc_epochs_test = mne.concatenate_epochs(list(sorted_proc_epoch.values())[-4:]).get_data()\n",
    "# all_raw_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Time-Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import tfr_morlet, psd_multitaper, psd_welch\n",
    "\n",
    "epochs_easy = raw_epochs_concat['easy'][20]\n",
    "epochs_hard = raw_epochs_concat['hard'][20]\n",
    "\n",
    "# freq_range = np.logspace(*np.log10([4, 55]), num=15)\n",
    "freq_range = np.linspace(4, 56, 28)\n",
    "n_cycles = freq_range / 2.\n",
    "\n",
    "power_easy, itc_easy = tfr_morlet(epochs_easy, freqs=freq_range, n_cycles=n_cycles, use_fft=True, return_itc=True, n_jobs=1)\n",
    "power_hard, itc_hard = tfr_morlet(epochs_hard, freqs=freq_range, n_cycles=n_cycles, use_fft=True, return_itc=True, n_jobs=1)\n",
    "\n",
    "sel_chan = 15\n",
    "\n",
    "# power_easy.plot_topo(baseline=(-0.5, -.2), mode='mean', title='Average power')\n",
    "# power_easy.plot([sel_chan], baseline=(-3.2, -3), mode='mean', title=power_easy.ch_names[sel_chan])\n",
    "power_easy.plot([sel_chan], baseline=(-, mode='mean', title=power_easy.ch_names[sel_chan])\n",
    "# power_easy.plot([sel_chan], baseline=None, mode='mean', title=power_easy.ch_names[sel_chan])\n",
    "\n",
    "# power_hard.plot_topo(baseline=None, mode='mean', title='Average power')\n",
    "power_hard.plot([sel_chan], baseline=None, mode='mean', title=power_hard.ch_names[sel_chan])\n",
    "# power_hard.plot([sel_chan], baseline=None, mode='mean', title=power_hard.ch_names[sel_chan])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Extraction and Classification (All Participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_dfs_cleaned_up = all_dfs.copy()\n",
    "all_dfs_cleaned_up = all_dfs_cleaned_up[(all_dfs_cleaned_up.spoken_difficulty_encoded != 0) & \n",
    "                                        (all_dfs_cleaned_up.spoken_difficulty_encoded.notnull())]\n",
    "all_dfs_cleaned_up = clean_up_adadrive_trials(all_dfs_cleaned_up)\n",
    "\n",
    "pupil_df = pd.read_csv(f\"../output/pupil_exposure/participant_level_exposure_fits.csv\")\n",
    "all_dfs_cleaned_up['Raw Left Pupil Diameter'] = all_dfs_cleaned_up['Left Pupil Diameter']\n",
    "p_val_criteria = 0.05\n",
    "\n",
    "for index, row in all_dfs_cleaned_up.reset_index(drop=True).iloc[1:].iterrows():\n",
    "    last_ppid = all_dfs_cleaned_up.iloc[index-1].ppid\n",
    "    last_session = all_dfs_cleaned_up.iloc[index-1].session\n",
    "    last_trial = all_dfs_cleaned_up.iloc[index-1].trial\n",
    "    last_opacity = all_dfs_cleaned_up.iloc[index-1].density\n",
    "    if ((row.ppid == last_ppid) & (row.session == last_session) & (row.trial == last_trial+1)): # if continuous\n",
    "        # if there is a significant effect of opacity on pupil\n",
    "        if pupil_df.loc[pupil_df['sub']==last_ppid,'p_opacities'].values < p_val_criteria:\n",
    "            this_opacity = row.density\n",
    "            this_pupil_diameter = row['Left Pupil Diameter']\n",
    "            weight = pupil_df.loc[pupil_df['sub']==last_ppid,'w_opacities']\n",
    "            adjustment = (this_opacity-last_opacity)*weight\n",
    "            all_dfs_cleaned_up.iloc[index,all_dfs_cleaned_up.columns.get_loc('Left Pupil Diameter')] -= adjustment\n",
    "            \n",
    "all_dfs_cleaned_up = all_dfs_cleaned_up[all_dfs_cleaned_up['Left Pupil Diameter'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EEG Features Extraction\n",
    "raw_eeg_features = eeg_features(all_dfs_cleaned_up, 'raw', label_source = 'spoken_difficulty_encoded', cleaned_up = False)\n",
    "processed_eeg_features = eeg_features(all_dfs_cleaned_up, 'processed', label_source = 'spoken_difficulty_encoded', cleaned_up = False)\n",
    "\n",
    "pupil_dia = eye_features(all_dfs_cleaned_up, features = ['Left Pupil Diameter'], label_source = 'spoken_difficulty_encoded', cleaned_up = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_raw_data = np.asarray(pd.concat([raw_eeg_features, pupil_dia], axis = 1))\n",
    "x_proc_data = np.asarray(pd.concat([processed_eeg_features, pupil_dia], axis = 1))\n",
    "y_labels = np.asarray(all_dfs_cleaned_up['spoken_difficulty_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EEG Features Normalization and Data Split\n",
    "proc_train_norm, proc_test_norm, y_train_proc, y_test_proc = feature_normalization(x_proc_data, y_labels)\n",
    "raw_train_norm, raw_test_norm, y_train_raw, y_test_raw = feature_normalization(x_raw_data, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trial Difficulty Classification - Raw EEG \n",
    "AUC_train_raw, AUC_test_raw, coefs_raw = trial_classification(raw_train_norm, raw_test_norm,\n",
    "                                                                          y_train_raw, y_test_raw, \n",
    "                                                                          'logistic', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trial Difficulty Classification - Artifacts Removed EEG\n",
    "AUC_train_proc, AUC_test_proc, coefs_proc = trial_classification(proc_train_norm, proc_test_norm,\n",
    "                                                                             y_train_proc, y_test_proc, \n",
    "                                                                             'logistic', 'processed')\n",
    "\n",
    "# print(f\"Training Accuracy with Artifacts Removal: {train_acc_proc:.2f} \\n\"\n",
    "#       f\"Training Label:      {y_train_proc} \\n\"\n",
    "#       f\"Training Prediction: {train_pred_proc} \\n\"\n",
    "#       f\"Test Accuracy with Artifacts Removal: {test_acc_proc:.2f} \\n\"\n",
    "#       f\"Test Label:      {y_test_proc} \\n\"\n",
    "#       f\"Test Prediction: {test_pred_proc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Important Features Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate table for feature importance\n",
    "\n",
    "# features_list = list(processed_selected_channel_bp.columns)\n",
    "features_list = list(processed_eeg_features.columns)\n",
    "features_list.extend([\"Left Pupil Diameter\",\"Right Pupil Diameter\"])\n",
    "\n",
    "bottom_10_raw = zip(np.argsort(coefs_raw)[:10], np.sort(coefs_raw)[:10])\n",
    "bottom_10_raw_features = [features_list[i] + f\" - {importance:.2E}\" for i, importance in bottom_10_raw]\n",
    "top_10_raw =  zip(np.argsort(coefs_raw)[-10:], np.sort(coefs_raw)[-10:])\n",
    "top_10_raw_features = [features_list[i] + f\" - {importance:.2E}\" for i, importance in top_10_raw]\n",
    "\n",
    "\n",
    "bottom_10_proc = zip(np.argsort(coefs_proc)[:10], np.sort(coefs_proc)[:10])\n",
    "bottom_10_proc_features = [features_list[i] + f\" - {importance:.2E}\" for i, importance in bottom_10_proc]\n",
    "top_10_proc = zip(np.argsort(coefs_proc)[-10:],np.sort(coefs_proc)[-10:])\n",
    "top_10_proc_features = [features_list[i] + f\" - {importance:.2E}\" for i, importance in top_10_proc]\n",
    "\n",
    "features_ranking = {\n",
    "    'Top 10 features for raw filtered data': top_10_raw_features,\n",
    "    'Bottom 10 features for raw filtered data': bottom_10_raw_features,\n",
    "    'Top 10 features for ICA processed raw filtered data': top_10_proc_features,\n",
    "    'Bottom 10 features for ICA processed raw filtered data': bottom_10_proc_features\n",
    "}\n",
    "\n",
    "df_features = pd.DataFrame(features_ranking, index = ['Low End','2','3','4','5','6','7','8','9','High End'])\n",
    "# df_features.to_excel(f\"{output_dir}saved_files/ranked_features_all.xlsx\")\n",
    "df_features.to_csv(f\"{output_dir}saved_files/ranked_features_all.csv\")\n",
    "\n",
    "# print (len(top_10_raw_features),len(bottom_10_raw_features), len(top_10_proc_features), len(bottom_10_proc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Within participant classification (Spoken Difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_clf = 'random_forest'\n",
    "participants_id = np.sort(all_dfs_cleaned_up.ppid.unique().astype(int)) \n",
    "\n",
    "participant_AUC = np.empty([len(participants_id),4])\n",
    "\n",
    "for i, participant in enumerate(participants_id):\n",
    "    single_sbj_df = all_dfs_cleaned_up[all_dfs_cleaned_up.ppid == participant]\n",
    "    \n",
    "    if np.sum((single_sbj_df['spoken_difficulty']=='easy') | (single_sbj_df['spoken_difficulty']=='hard')) < 10:\n",
    "        AUC_train_raw = AUC_test_raw = AUC_train_proc = AUC_test_proc = float('NaN')\n",
    "    else:\n",
    "        raw_eeg_features, raw_data_features, raw_labels = eeg_features(single_sbj_df, 'raw')\n",
    "        processed_eeg_features, processed_data_features, processed_labels = eeg_features(single_sbj_df, 'processed')\n",
    "\n",
    "        raw_train_norm, raw_test_norm, proc_train_norm, proc_test_norm, y_train_raw, y_test_raw, y_train_proc, y_test_proc = feature_normalization(\n",
    "                                                            processed_data_features, processed_labels, raw_data_features, raw_labels, train_percentage = 0.9)\n",
    "\n",
    "        AUC_train_raw, AUC_test_raw, coefs_raw = trial_classification(raw_train_norm, raw_test_norm,\n",
    "                                                                                  y_train_raw, y_test_raw, \n",
    "                                                                                  selected_clf, 'raw', save_plots=False)\n",
    "        AUC_train_proc, AUC_test_proc, coefs_proc = trial_classification(proc_train_norm, proc_test_norm,\n",
    "                                                                                     y_train_proc, y_test_proc, \n",
    "                                                                                     selected_clf, 'processed', save_plots=False)\n",
    "\n",
    "    participant_AUC[i,:] = [AUC_train_raw, AUC_test_raw, AUC_train_proc, AUC_test_proc]\n",
    "\n",
    "participant_AUC_df = pd.DataFrame(participant_AUC, index = participants_id, \n",
    "                                  columns = ['Raw Train AUC', 'Raw Test AUC', 'ICA Processed Train AUC', 'ICA Processed Test AUC'])\n",
    "\n",
    "participant_AUC_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Processing and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_output_dir = (f\"../output/batch_analysis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "def str_list_to_list(lst):\n",
    "    str_single_space = re.sub(\"\\s+\", \" \", lst.strip())\n",
    "    str_no_brackets = re.sub(\"[\\[\\]]\", \"\", lst)\n",
    "    return [float(n) for n in str_no_brackets.split()]\n",
    "\n",
    "# loop over the list of csv files\n",
    "def read_motor_csvs():\n",
    "    csv_files = glob.glob(os.path.join(motor_output_dir, \"ppid*_motor.csv\"))\n",
    "    all_dfs = None\n",
    "    for f in csv_files:\n",
    "        # read the csv file and add column for labels\n",
    "        temp_df = pd.read_csv(f)\n",
    "\n",
    "        all_steer_events = temp_df.copy()['post_steer_event_raw']\n",
    "        all_steer_events_finalized = all_steer_events.apply(str_list_to_list)\n",
    "\n",
    "        norm_pos = lambda wheel_pos: np.asarray(wheel_pos)/np.asarray(wheel_pos[0])\n",
    "        final_pos = lambda final_wheel_pos: np.asarray(final_wheel_pos[-1])-np.asarray(final_wheel_pos[0])\n",
    "\n",
    "        norm_pos_df = all_steer_events_finalized.apply(norm_pos)\n",
    "\n",
    "        temp_df['Steering_Wheel_Degree'] = abs(all_steer_events_finalized.apply(final_pos))\n",
    "        temp_df['Steering_Wheel_Degree_Categorical'] = pd.qcut(temp_df['Steering_Wheel_Degree'], 2, labels=[\"Low\", \"High\"]) #2=High, 1 =Low\n",
    "        temp_df['Steering_Wheel_Degree_Encoded'] = temp_df.Steer_Wheel_Degree_Categorical.replace({'High': 2, 'Low': 1})\n",
    "        # temp_df['Mean_Steer_Wheel_Degree'] = temp_df.Steer_Wheel_Degree.mean()\n",
    "        \n",
    "        if not type(all_dfs)==pd.core.frame.DataFrame:\n",
    "            all_dfs = temp_df\n",
    "        else:\n",
    "            all_dfs = pd.concat([all_dfs, temp_df], ignore_index=True)\n",
    "            \n",
    "    all_dfs = all_dfs[all_dfs.columns.drop(list(all_dfs.filter(regex='Unnamed')))]\n",
    "    \n",
    "    return all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs_all_pp_trials = read_motor_csvs()\n",
    "motor_all_dfs = all_dfs_all_pp_trials.copy()\n",
    "\n",
    "motor_all_dfs['sub_sess'] = motor_all_dfs.ppid.astype(str) + \"_\" + motor_all_dfs.session.astype(str)\n",
    "motor_all_dfs = motor_all_dfs.loc[~motor_all_dfs.sub_sess.isin([f\"{es[0]}.0_{es[1]}.0\" for es in remove_sessions])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_all_dfs = clean_up_adadrive_trials(motor_all_dfs)\n",
    "motor_all_dfs['Mean_Steering_Wheel_Degree'] = motor_all_dfs.Steering_Wheel_Degree.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=motor_all_dfs, x=\"Steering_Wheel_Degree\")\n",
    "# plt.savefig(f\"../output/plots/steering_wheel_turned_deg.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_all_dfs.to_csv(f\"../output/batch_analysis/motor_df_label.csv\")\n",
    "# motor_all_dfs.to_excel(f\"../output/batch_analysis/motor_df_label.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Extraction and Classification - 10 folds CV (Pupil, ECG, EEG, All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eeg, y_eeg = eeg_features(motor_all_dfs, cleaned_up = True)\n",
    "x_eye, y_eye = eye_features(motor_all_dfs, cleaned_up = True)\n",
    "x_ecg, y_ecg = ecg_features(motor_all_dfs, cleaned_up = True)\n",
    "x_all, y_all = multimodal_features(motor_all_dfs)\n",
    "\n",
    "modality_dict = {\"EEG\": (x_eeg, y_eeg),\n",
    "                 \"Eye\": (x_eye, y_eye),\n",
    "                 \"ECG\": (x_ecg, y_ecg),\n",
    "                 \"All\": (x_all, y_all)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_all.shape, x_eeg.shape, x_eye.shape, x_ecg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_auc_logreg = {}\n",
    "\n",
    "for modality in list(modality_dict.keys()):\n",
    "\n",
    "    modality_auc_logreg[modality] = modality_cv(modality_dict[modality][0], modality_dict[modality][1],\n",
    "                                        classifier = 'logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(modality_auc_logreg, index = ['Train AUC', 'Test AUC'])\n",
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_auc_rf = {}\n",
    "\n",
    "for modality in list(modality_dict.keys()):\n",
    "\n",
    "    modality_auc_rf[modality] = modality_cv(modality_dict[modality][0], modality_dict[modality][1],\n",
    "                                        classifier = 'random_forest')\n",
    "    \n",
    "pd.DataFrame(modality_auc_rf, index = ['Train AUC', 'Test AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modality_cv(x_ecg, y_ecg, classifier = 'random_forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Feature Extraction and Classification (All Participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_power, beta_power, theta_hjorth_activity, theta_hfd, theta_power, theta_sample_entropy,\n",
    "# theta_hjorth_mobility, alpha_hjorth_activity, alpha_sample_entropy, alpha_hfd, beta_hjorth_activity,\n",
    "# beta_hjorth_activity, gamma_power, gamma_hjorth_activity, beta_hfd, beta_hjorth_complexity,\n",
    "# beta_hjorth_mobility, alpha_hjorth_mobility, gamma_hfd, alpha_hjorth_complexity,\n",
    "# theta_hjorth_complexity, beta_sample_entropy, gamma_hjorth_mobility, gamma_hjorth_complexity,\n",
    "# gamma_sample_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature = '8-15_Hz_Power|15-32_Hz_Power|4-8_Hz_Hjorth_Activity|4-8_Hz_Higuchi_FD|4-8_Hz_Power|4-8_Hz_Sample_entropy|4-8_Hz_Hjorth_Mobility|8-13_Hz_Hjorth_Activity|8-13_Hz_Sample_entropy| 8-13_Hz_Higuchi_FD'\n",
    "\n",
    "processed_eeg_features, processed_data_features, processed_labels = eeg_features(motor_all_dfs, 'processed', \n",
    "                                                                                 selected_chans = False, features = 'all',\n",
    "                                                                                 label_source = 'Steer_Wheel_Degree_Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, proc_train_norm, proc_test_norm, _, _, y_train_proc, y_test_proc = feature_normalization(processed_data_features,\n",
    "                                                                                               processed_labels,\n",
    "                                                                                               raw_feature = None, \n",
    "                                                                                               raw_labels = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUC_train_proc, AUC_test_proc, coefs_proc = trial_classification(proc_train_norm, proc_test_norm,\n",
    "                                                                             y_train_proc, y_test_proc, \n",
    "                                                                             'logistic', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUC_train_proc, AUC_test_proc, coefs_proc = trial_classification(proc_train_norm, proc_test_norm,\n",
    "                                                                             y_train_proc, y_test_proc, \n",
    "                                                                             'random_forest', 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Within Participant Classification (Steering Wheel Position Difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_clf = 'random_forest'\n",
    "participants_id = np.sort(motor_all_dfs.ppid.unique().astype(int)) \n",
    "\n",
    "participant_AUC = np.empty([len(participants_id),2])\n",
    "\n",
    "for i, participant in enumerate(participants_id):\n",
    "    single_sbj_df = motor_all_dfs[motor_all_dfs.ppid == participant]\n",
    "\n",
    "    processed_eeg_features, processed_data_features, processed_labels = eeg_features(single_sbj_df, 'processed',\n",
    "                                                                                    label_source = 'Steer_Wheel_Degree_Encoded')\n",
    "\n",
    "    _, _, proc_train_norm, proc_test_norm, _, _, y_train_proc, y_test_proc = feature_normalization(processed_data_features, \n",
    "                                                                                                   processed_labels, \n",
    "                                                                                                   raw_feature = None, \n",
    "                                                                                                   raw_labels = None)\n",
    "\n",
    "    AUC_train_proc, AUC_test_proc, coefs_proc = trial_classification(proc_train_norm, proc_test_norm,\n",
    "                                                                     y_train_proc, y_test_proc, \n",
    "                                                                     selected_clf, 'processed', save_plots=False)\n",
    "\n",
    "    participant_AUC[i,:] = [AUC_train_proc, AUC_test_proc]\n",
    "\n",
    "\n",
    "participant_AUC_df = pd.DataFrame(participant_AUC, index = participants_id, \n",
    "                                  columns = ['ICA Processed Train AUC', 'ICA Processed Test AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant_AUC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def cal_rmse(df, true_val_col = 'Steer_Wheel_Degree', eeg_features_list = 'all', \n",
    "#              eye_features_list = 'all', ecg_features_list = 'all'):\n",
    "    \n",
    "#     if eeg_features == 'all':\n",
    "#         # all eeg features rmse calculation\n",
    "#         x_eeg, _, _ = eeg_features(df, 'processed', label_source = 'Steer_Wheel_Degree_Encoded')\n",
    "#     else:\n",
    "#         # selected eeg features rmse calculation\n",
    "#         # selected_feature = '8-15_Hz_Power|15-32_Hz_Power|4-8_Hz_Hjorth_Activity|4-8_Hz_Higuchi_FD|4-8_Hz_Power|4-8_Hz_Sample_entropy|4-8_Hz_Hjorth_Mobility|8-13_Hz_Hjorth_Activity|8-13_Hz_Sample_entropy| 8-13_Hz_Higuchi_FD'\n",
    "#         selected_feature = \"|\".join(map(str,eeg_features_list))\n",
    "#         x_eeg, _, _ = eeg_features(df, 'processed', features = selected_feature, label_source = 'Steer_Wheel_Degree_Encoded')\n",
    "#         y_eeg = df[true_val_col]\n",
    "        \n",
    "#     X_eeg_train, X_eeg_test, y_eeg_train, y_eeg_test = train_test_split(x_eeg, y_eeg, test_size=0.3, random_state=rs)\n",
    "#     regr.fit(X_eeg_train, y_eeg_train)\n",
    "\n",
    "#     y_eeg_pred = regr.predict(X_eeg_test)\n",
    "#     eeg_rmse = mean_squared_error(y_eeg_test, y_eeg_pred, squared=False)\n",
    "    \n",
    "#     # eye rmse calculation\n",
    "#     if eye_features_list == 'all':\n",
    "#         eye_features = [\"Left Pupil Diameter\", \"Right Pupil Diameter\",\n",
    "#                    \"Left Evoked Pupil Diameter\", \"Right Evoked Pupil Diameter\", true_val_col]\n",
    "#     else:\n",
    "#         eye_features = eye_features_list\n",
    "#         eye_features.append(true_val_col)\n",
    "     \n",
    "#     eye_df = df[eye_features].dropna()\n",
    "#     x_eye = eye_df.iloc[:,0:-1]\n",
    "#     y_eye = eye_df.iloc[:,-1]\n",
    "#     X_eye_train, X_eye_test, y_eye_train, y_eye_test = train_test_split(x_eye, y_eye, test_size=0.3, random_state=rs)\n",
    "#     regr.fit(X_eye_train, y_eye_train)\n",
    "\n",
    "#     y_eye_pred = regr.predict(X_eye_test)\n",
    "#     eye_rmse = mean_squared_error(y_eye_test, y_eye_pred, squared=False)\n",
    "    \n",
    "#     #ecg emse calculation\n",
    "#     if ecg_features_list == 'all':\n",
    "#         ecg_feature_first = df.columns.get_loc(\"bpm\")\n",
    "#         ecg_feature_last = df.columns.get_loc(\"breathingrate\")\n",
    "#         ecg_feature = df.iloc[:,ecg_feature_first:ecg_feature_last-2].join(df[true_val_col])\n",
    "#         ecg_df = ecg_feature.dropna()\n",
    "#     else:\n",
    "#         ecg_feature = ecg_features_list\n",
    "#         ecg_feature.append(true_val_col)\n",
    "#         ecg_df = df[ecg_feature].dropna()\n",
    "        \n",
    "#     x_ecg = ecg_df.iloc[:,0:-1]\n",
    "#     y_ecg = ecg_df.iloc[:,-1]\n",
    "#     X_ecg_train, X_ecg_test, y_ecg_train, y_ecg_test = train_test_split(x_ecg, y_ecg, test_size=0.3, random_state=rs)\n",
    "\n",
    "#     regr.fit(X_ecg_train, y_ecg_train)\n",
    "#     y_ecg_pred = regr.predict(X_ecg_test)\n",
    "#     ecg_rmse = mean_squared_error(y_ecg_test, y_ecg_pred, squared=False)\n",
    "    \n",
    "#     return y_eeg_test, y_eeg_pred, eeg_rmse, \n",
    "#         y_eye_test, y_eye_pred, eye_rmse, \n",
    "#         y_ecg_test, y_ecg_pred, ecg_rmse\n",
    "\n",
    "\n",
    "# y_eeg_test, y_eeg_pred, eeg_rmse, \n",
    "#             y_eye_test, y_eye_pred, eye_rmse, \n",
    "#             y_ecg_test, y_ecg_pred, ecg_rmse = cal_rmse(motor_all_dfs, true_val_col = 'Steering_Wheel_Degree',\n",
    "#                                                         eeg_features_list = ['F1_15-32_Hz_Power'],\n",
    "#                                                         eye_features_list = ['Left Pupil Diameter'],\n",
    "#                                                         ecg_features_list = ['sdnn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mean = mean_squared_error(motor_all_dfs['Steering_Wheel_Degree'], \n",
    "                               motor_all_dfs['Mean_Steering_Wheel_Degree'], squared=False)\n",
    "rmse_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modality_rmse_dict = {\"EEG\": [], \"Eye\": [], \"ECG\": [], \"All\": []}\n",
    "\n",
    "# _,_,eeg_rmse = calculate_rmse(motor_all_dfs, 'EEG', features_list = ['F1_15-32_Hz_Power'])\n",
    "\n",
    "for modality in list(modality_rmse_dict.keys()):\n",
    "    _,_,rmse = calculate_rmse(motor_all_dfs, modality)\n",
    "    modality_rmse_dict[modality] = rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = pd.DataFrame(modality_rmse_dict, index = ['rmse'])\n",
    "\n",
    "results_clf_reg = pd.concat([rmse_df, auc_df], axis = 0)\n",
    "# results_clf_reg.to_csv(f'../output/saved_files/all_modality_rmse_auc.csv')\n",
    "results_clf_reg.to_excel(f'../output/saved_files/all_modality_rmse_auc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_rmse, eye_rmse, ecg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_data = [[eeg_rmse_all_mean, eeg_rmse_selected_mean, eye_rmse_mean, ecg_rmse_mean],\n",
    "        [eeg_rmse_all, eeg_rmse_selected, eye_rmse, ecg_rmse]]\n",
    "rmse_column = ['EEG prediction RMSE - all features', 'EEG prediction RMSE - selected features', 'Eye prediction RMSE', 'ECG prediction RMSE']\n",
    "rmse_row = ['Mean', 'Raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = pd.DataFrame(rmse_data, index = rmse_row, columns = rmse_column)\n",
    "rmse_df.to_csv(f'../output/saved_files/all_modality_rmse.csv')\n",
    "rmse_df.to_excel(f'../output/saved_files/all_modality_rmse.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = dict({\"EEG_all_Mean_pred\": y_eeg_all_pred_mean,\n",
    "                  \"EEG_selected_Mean_pred\": y_eeg_pred_mean,  \n",
    "                  \"Eye_Mean_pred\": y_eye_pred_mean,\n",
    "                  \"ECG_Mean_pred\": y_ecg_pred_mean,\n",
    "                  \"EEG_all_pred\": y_eeg_all_pred, \n",
    "                  \"EEG_selected_pred\": y_eeg_pred, \n",
    "                  \"Eye_pred\": y_eye_pred,\n",
    "                  \"ECG_pred\": y_ecg_pred, \n",
    "                 })\n",
    "\n",
    "with open('../output/saved_files/pickle_files/modality_pred.pickle', 'wb') as handle_pred:\n",
    "            pickle.dump(pred_dict, handle_pred, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = dict({\"EEG_all_Mean_test\": y_eeg_all_test_mean, \n",
    "                  \"EEG_selected_Mean_test\": y_eeg_test_mean, \n",
    "                  \"Eye_Mean_test\": y_eye_test_mean,\n",
    "                  \"ECG_Mean_test\": y_ecg_test_mean,\n",
    "                  \"EEG_all_test\": y_eeg_all_test, \n",
    "                  \"EEG_selected_test\": y_eeg_test, \n",
    "                  \"Eye_test\": y_eye_test,\n",
    "                  \"ECG_test\": y_ecg_test,\n",
    "                 })\n",
    "\n",
    "with open('../output/saved_files/pickle_files/modality_test.pickle', 'wb') as handle_test:\n",
    "            pickle.dump(test_dict, handle_test, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new classification label based on wheel input\n",
    "\n",
    "wheel_input_metric = all_dfs_cleaned_up['abs_sum_delta_throttle_input']/all_dfs_cleaned_up['abs_sum_delta_brake_input']\n",
    "\n",
    "driving_metric = pd.qcut(wheel_input_metric, 2, labels=[\"High\", \"Low\"])\n",
    "driving_metric_df = pd.DataFrame(driving_metric,columns = ['driving_metric'])\n",
    "all_dfs_cleaned_up = all_dfs_cleaned_up.join(driving_metric_df)\n",
    "\n",
    "all_dfs_cleaned_up = all_dfs_cleaned_up.replace({'driving_metric':{'High': 2, 'Low': 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install EMD_signal\n",
    "import PyEMD\n",
    "from PyEMD import EMD, Visualisation\n",
    "\n",
    "# eeg_comps = ica.get_sources(raw).get_data() #eeg componenets for all epochs\n",
    "# # eeg_comps = ica.get_sources(epochs).get_data() #eeg componenets for all epochs\n",
    "# # comps_epoch_concat = np.empty([eeg_comps.shape[1],eeg_comps.shape[2]*eeg_comps.shape[0]]) #initiate empty array\n",
    "# # for i in range(eeg_comps.shape[0]):\n",
    "# #     comps_epoch_concat[:,i*eeg_comps.shape[2]:eeg_comps.shape[2]*(i+1)] = eeg_comps[i]\n",
    "\n",
    "# component_no = 5\n",
    "# test_comps = eeg_comps[component_no]\n",
    "\n",
    "# emd = EMD() # EMD instantiation\n",
    "# emd.emd(test_comps) # decompose signal into IMFs and residue\n",
    "# imfs, res = emd.get_imfs_and_residue()\n",
    "\n",
    "# # # imfs = emd(np.squeeze(eeg_comps[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scipy.stats.kurtosis(test_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Visualization\n",
    "# t = np.arange(0, 3+1/freq, 1/freq)\n",
    "# vis = Visualisation()\n",
    "# vis.plot_imfs(imfs=imfs, residue=res, t=t, include_residue=True)\n",
    "# # vis.plot_instant_freq(t, imfs=imfs)\n",
    "# vis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "mna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "967869b3d3e599d39c4e482f6852385da2dcc34e629cb6c89bbd952eba61abed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
