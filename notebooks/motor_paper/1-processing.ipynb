{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96671aa",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d0746",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "path_to_base_package = '../..'\n",
    "import sys\n",
    "\n",
    "# setting path\n",
    "sys.path.append(f\"{path_to_base_package}\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from mna.sessions.eye_session import process_session_eye\n",
    "from mna.sessions.eeg_session import process_session_eeg\n",
    "from mna.sessions.motor_session import process_session_motor\n",
    "from mna.sessions.ecg_session import process_session_ecg\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from mna.utils.rnapp_data_format import read_all_lslpresets, return_metadata_from_name, event_data_from_data, read_event_data\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from pivottablejs import pivot_ui\n",
    "from mna.utils.rnapp_data_format import read_all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13166b",
   "metadata": {},
   "source": [
    "# Batch convert raw files into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae990260",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reconvert_raw_filse = False # note that this conversion needs to only be done once\n",
    "if reconvert_raw_filse:\n",
    "    read_all_files(data_dir='/home/jupyter/raw_data/',\n",
    "                   pickle_dir='/home/jupyter/mna/data/', save_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf1d07",
   "metadata": {},
   "source": [
    "# Batch analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4865e6a1-51db-4e99-a973-f1421e0b648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"{path_to_base_package}/data/\"\n",
    "timestamp_fixer_path = f\"{data_dir}annotated/fit_timestamp_adjuster.pkl\"\n",
    "lsl_dir = f\"{path_to_base_package}/mna/LSLPresets/\"\n",
    "output_dir = f\"{path_to_base_package}/output/batch_analysis_non_baseline/\"\n",
    "if not os.path.isdir(output_dir): os.makedirs(output_dir)\n",
    "metadata_jsons = read_all_lslpresets(path_to_jsonfiles=lsl_dir)\n",
    "onlyfiles = [f for f in listdir(data_dir) if isfile(join(data_dir, f)) and '.pkl' in f]\n",
    "ts_fixer = pickle.load(open(timestamp_fixer_path, 'rb')) # features == 'processed_trial_duration',  'processed_trial_duration_1', 'lsl_timestamps'\n",
    "interrupted_sessions = [(13,1), (22,1)]\n",
    "remove_sessions = [(13,1),(15,1),(22,1)]\n",
    "reference_ica = \"sbj20ssn03\"\n",
    "save_data_pkl = True # save data into pickle files\n",
    "save_ica_plts = False # save ICA components plots\n",
    "epoch_raw_eeg = False # epoching raw data\n",
    "\n",
    "preturn = 1000\n",
    "baseline_period = 250\n",
    "rs = 64 # random seed\n",
    "\n",
    "# determine whether we do motor analysis\n",
    "motor_events = True # this determines whether we do baseline correction and average reference\n",
    "if motor_events:\n",
    "    set_baseline=None\n",
    "    set_average_reference=False\n",
    "else:\n",
    "    set_baseline=(-((preturn+250)/1000),-((preturn)/1000))\n",
    "    set_average_reference=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb4ac6-1908-4534-a39d-78138f58f6a2",
   "metadata": {},
   "source": [
    "## Process file to extract features for all modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9e7e4e-640b-4dfc-b440-6f6e3982987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(ica_dict, each_file, overwrite=False):\n",
    "    ica_epochs_dict = {}\n",
    "    eog_idx_dict = {}\n",
    "    events_dict = {}\n",
    "    input_path = data_dir + each_file\n",
    "\n",
    "    sbj_id = each_file[each_file.find('Sbj_')+4:each_file.find('-Ssn')]\n",
    "    ssn_no = each_file[each_file.find('Ssn_')+4:each_file.find('.dats')]\n",
    "\n",
    "    if len(sbj_id) < 2: sbj = \"sbj0\"+sbj_id\n",
    "    else: sbj = \"sbj\"+sbj_id\n",
    "    if len(ssn_no) < 2: ssn = \"ssn0\"+ssn_no\n",
    "    else: ssn = \"ssn\"+ssn_no\n",
    "    if reference_ica in ica_dict:\n",
    "        ref_ica = ica_dict['sbj20ssn03']\n",
    "    elif sbj+ssn == reference_ica:\n",
    "        ref_ica = None\n",
    "    if not overwrite and os.path.exists(f\"{output_dir}/saved_files/{sbj+ssn}/\"):\n",
    "        print(sbj+ssn,'sbj+ssn already exists')\n",
    "        return None\n",
    "\n",
    "    with open(input_path, 'rb') as handle:\n",
    "        rns_data = pickle.load(handle)\n",
    "\n",
    "    ## Add metadata to data\n",
    "\n",
    "    for key in rns_data.keys():\n",
    "        rns_data[key].append(return_metadata_from_name(key, metadata_jsons))\n",
    "\n",
    "    event_df = read_event_data(rns_data, remove_id_sessions=remove_sessions) # typically only 15_1 and 22_1 will be used here, change below too\n",
    "    if event_df.empty:\n",
    "        return None\n",
    "    event_df = event_df[event_df.block_condition == 'voice']\n",
    "    event_df['trial_damage'] = event_df.damage.diff().fillna(0)\n",
    "    event_df['trial_duration'] = event_df.trial_end_time - event_df.trial_start_time\n",
    "\n",
    "    percent_missing = event_df.notnull().sum() / len(event_df)\n",
    "    summary_statistics = {}\n",
    "    summary_statistics['voice_success_rate'] = percent_missing['spoken_difficulty']\n",
    "    event_df['spoken_difficulty'] = event_df['spoken_difficulty'].fillna(\"unknown\")\n",
    "    event_df['spoken_difficulty_encoded'] = event_df.spoken_difficulty.replace(to_replace=['easy', 'hard', 'unknown'],\n",
    "                                                                               value=[1, 2, 0])\n",
    "\n",
    "    # motor\n",
    "    post_processed_event_df, turns_df = process_session_motor(rns_data, event_df, motor_channel='Unity_MotorInput', plot_motor_result = False, plot_motor_snippet = 30, plot_frequency = 0, preturn=preturn)\n",
    "    if motor_events:\n",
    "        post_processed_event_df = turns_df\n",
    "\n",
    "    # ecg\n",
    "    # fit and report on 60 seconds leading up to event start (minimum required for HF component)\n",
    "    post_processed_event_df = process_session_ecg(rns_data, post_processed_event_df,plot_frequency=0,plot_ecg_snippet=40,pretrial_period=30)\n",
    "\n",
    "    if 'Unity_ViveSREyeTracking' in rns_data:\n",
    "        # fit on 3 seconds before event start, but only report data on segments during the period\n",
    "        post_processed_event_df = process_session_eye(rns_data, post_processed_event_df,detect_blink=True, pretrial_period=3, posttrial_period=0, plot_frequency=0, plot_eye_snippet=40, classifiers=['NSLR'], pupil_average_limit=False)\n",
    "\n",
    "    # eeg\n",
    "    post_processed_event_df, epochs, events, info, reject_log, ica, eog_idx = process_session_eeg(rns_data, post_processed_event_df, run_autoreject=True, run_ica=True, save_raw_eeg = False, sbj_session = sbj+ssn, plot_epochs = False, template_ica = ref_ica, analyze_pre_ica = False, average_reference=set_average_reference, eye_movement_removal=False, tmin=-((preturn+baseline_period)/preturn), tmax=0, baseline = set_baseline, normalize_pow_freq = True, filter_events = False)\n",
    "    ica_dict[sbj+ssn] = ica\n",
    "\n",
    "    if motor_events:\n",
    "        post_processed_event_df.to_csv(f\"{output_dir}ppid_{post_processed_event_df.iloc[0].ppid}_session_{post_processed_event_df.iloc[0].session}_motor.csv\")\n",
    "    else:\n",
    "        post_processed_event_df.to_csv(f\"{output_dir}ppid_{post_processed_event_df.iloc[0].ppid}_session_{post_processed_event_df.iloc[0].session}.csv\")\n",
    "\n",
    "\n",
    "    # save data for later use\n",
    "    if save_data_pkl:\n",
    "        pickle_dir = f\"{output_dir}/saved_files/{sbj+ssn}/\"\n",
    "        os.makedirs(os.path.dirname(pickle_dir), exist_ok=True)\n",
    "        with open(f\"{pickle_dir}events.pickle\", 'wb') as handle_events:\n",
    "            pickle.dump(events, handle_events, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f\"{pickle_dir}ica_epochs.pickle\", 'wb') as handle_ica_eps:\n",
    "            pickle.dump(epochs, handle_ica_eps, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f\"{pickle_dir}ica.pickle\", 'wb') as handle_ica:\n",
    "            pickle.dump(ica_dict, handle_ica, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f\"{pickle_dir}eog_idx.pickle\", 'wb') as handle_eog:\n",
    "            pickle.dump(eog_idx, handle_eog, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return post_processed_event_df, events_dict, ica_epochs_dict, ica_dict, eog_idx_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set up the reference participant (20_3) for blink removal using ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2ad9a-bc09-4785-ae09-f013974384c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=89, n_times=3399947\n",
      "    Range : 0 ... 3399946 =      0.000 ...  1660.130 secs\n",
      "Ready.\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 13.75 Hz (-6 dB cutoff frequency: 61.88 Hz)\n",
      "- Filter length: 6759 samples (3.300 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 64 channels (please be patient, this may take a while)\n",
      "Selecting by number: 64 components\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "result = process_file({}, onlyfiles[0], set_baseline, set_average_reference, overwrite=True)\n",
    "ica_dict = result[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c5729-57e9-423a-8716-4168bae7d542",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_process_files = False\n",
    "if multi_process_files:\n",
    "    with Pool(4) as p:\n",
    "        results = p.map(partial(process_file, ica_dict), onlyfiles)\n",
    "else:\n",
    "    for onlyfile in onlyfiles[1:]:\n",
    "        result = process_file(ica_dict, onlyfile)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "all_dfs = pd.concat([r[0] for r in results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636cefe",
   "metadata": {},
   "source": [
    "# Save results and generate interactive PivotTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b116d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if motor_events:\n",
    "    all_dfs.to_csv(f\"{output_dir}all_results_motor.csv\", index=False)\n",
    "    all_dfs.to_excel(f\"{output_dir}all_results_motor.xlsx\")\n",
    "    pivot_ui(all_dfs, outfile_path=f\"{output_dir}all_results_motor.html\");\n",
    "else:\n",
    "    all_dfs.to_csv(f\"{output_dir}all_results.csv\", index=False)\n",
    "    all_dfs.to_excel(f\"{output_dir}all_results.xlsx\")\n",
    "    pivot_ui(all_dfs, outfile_path=f\"{output_dir}all_results.html\");"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "mna",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "mna",
   "language": "python",
   "name": "mna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
