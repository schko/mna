{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069667e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using notebook 3d backend.\n",
      "\n",
      "0 files missing from root.txt in /Users/schko/mne_data/MNE-fsaverage-data\n",
      "0 files missing from bem.txt in /Users/schko/mne_data/MNE-fsaverage-data/fsaverage\n",
      "Reading labels from parcellation...\n",
      "   read 82 labels from /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/label/lh.PALS_B12_Brodmann.annot\n",
      "   read 46 labels from /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/label/rh.PALS_B12_Brodmann.annot\n",
      "reading participant-level motor data\n",
      "found cleaned epochs\n",
      "Adding metadata with 5 columns\n",
      "509 matching events found\n",
      "No baseline correction applied\n",
      "removing ovlerlapping motor trials, starting epoch count 6291\n",
      "post removal epoch count 6218\n",
      "Replacing existing metadata with 75 columns\n",
      "Applying baseline correction (mode: mean)\n",
      "\\begin{tabular}{rrrr}\n",
      "\\toprule\n",
      " sub &  w\\_opacities &  const &  p\\_opacities \\\\\n",
      "\\midrule\n",
      "  20 &        -0.63 &   0.80 &         0.00 \\\\\n",
      "  14 &        -0.15 &   0.24 &         0.00 \\\\\n",
      "  12 &        -1.06 &   0.96 &         0.00 \\\\\n",
      "  22 &        -1.18 &   1.33 &         0.00 \\\\\n",
      "  16 &        -0.27 &   0.37 &         0.00 \\\\\n",
      "  18 &        -1.23 &   0.98 &         0.00 \\\\\n",
      "  19 &         0.34 &  -0.13 &         0.00 \\\\\n",
      "  21 &        -0.44 &   0.46 &         0.00 \\\\\n",
      "  15 &        -3.06 &   2.15 &         0.00 \\\\\n",
      "  13 &        -1.11 &   1.13 &         0.00 \\\\\n",
      "  23 &        -0.55 &   0.58 &         0.00 \\\\\n",
      "  17 &        -0.60 &   0.45 &         0.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Source space          : /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-ico-5-src.fif\n",
      "MRI -> head transform : /Users/schko/.conda/envs/mna/lib/python3.10/site-packages/mne/data/fsaverage/fsaverage-trans.fif\n",
      "Measurement data      : instance of Info\n",
      "Conductor model   : /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-5120-5120-5120-bem-sol.fif\n",
      "Accurate field computations\n",
      "Do computations in head coordinates\n",
      "Free source orientations\n",
      "\n",
      "Reading /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-ico-5-src.fif...\n",
      "Read 2 source spaces a total of 20484 active source locations\n",
      "\n",
      "Coordinate transformation: MRI (surface RAS) -> head\n",
      "     0.999994  0.003552  0.000202      -1.76 mm\n",
      "    -0.003558  0.998389  0.056626      31.09 mm\n",
      "    -0.000001 -0.056626  0.998395      39.60 mm\n",
      "     0.000000  0.000000  0.000000       1.00\n",
      "\n",
      "Read  64 EEG channels from info\n",
      "Head coordinate coil definitions created.\n",
      "Source spaces are now in head coordinates.\n",
      "\n",
      "Setting up the BEM model using /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-5120-5120-5120-bem-sol.fif...\n",
      "\n",
      "Loading surfaces...\n",
      "\n",
      "Loading the solution matrix...\n",
      "\n",
      "Three-layer model surfaces loaded.\n",
      "Loaded linear collocation BEM solution from /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-5120-5120-5120-bem-sol.fif\n",
      "Employing the head->MRI coordinate transform with the BEM model.\n",
      "BEM model fsaverage-5120-5120-5120-bem-sol.fif is now set up\n",
      "\n",
      "Source spaces are in head coordinates.\n",
      "Checking that the sources are inside the surface (will take a few...)\n",
      "Checking surface interior status for 10242 points...\n",
      "    Found  2433/10242 points inside  an interior sphere of radius   47.7 mm\n",
      "    Found     0/10242 points outside an exterior sphere of radius   98.3 mm\n",
      "    Found     0/ 7809 points outside using surface Qhull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found     0/ 7809 points outside using solid angles\n",
      "    Total 10242/10242 points inside the surface\n",
      "Interior check completed in 2373.2 ms\n",
      "Checking surface interior status for 10242 points...\n",
      "    Found  2241/10242 points inside  an interior sphere of radius   47.7 mm\n",
      "    Found     0/10242 points outside an exterior sphere of radius   98.3 mm\n",
      "    Found     0/ 8001 points outside using surface Qhull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found     0/ 8001 points outside using solid angles\n",
      "    Total 10242/10242 points inside the surface\n",
      "Interior check completed in 2259.1 ms\n",
      "\n",
      "Setting up for EEG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing EEG at 20484 source locations (free orientations)...\n"
     ]
    }
   ],
   "source": [
    "%run 2-source.ipynb\n",
    "low_motor_sensor = motor_epochs[\"Steer_Wheel_Degree_Categorical == 'Low'\"]\n",
    "high_motor_sensor = motor_epochs[\"Steer_Wheel_Degree_Categorical == 'High'\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a7a383c",
   "metadata": {},
   "source": [
    "# Functional connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "from mne.datasets import sample\n",
    "from mne import setup_volume_source_space, setup_source_space\n",
    "from mne import make_forward_solution\n",
    "from mne.io import read_raw_fif\n",
    "from mne.minimum_norm import make_inverse_operator, apply_inverse_epochs\n",
    "from mne.viz import circular_layout\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labels from parcellation...\n",
      "   read 82 labels from /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/label/lh.PALS_B12_Brodmann.annot\n",
      "   read 46 labels from /Users/schko/mne_data/MNE-fsaverage-data/fsaverage/label/rh.PALS_B12_Brodmann.annot\n"
     ]
    }
   ],
   "source": [
    "rel_labels, rel_mappings = get_relevant_labels_mappings(path_to_base_package) # for all regions use 'all'\n",
    "for l in rel_labels:\n",
    "    l.name = f\"{rel_mappings[l.name]}-{l.hemi}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556dc6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connectivity_plot(rel_stcs, output_dir, fmin, fmax, fig_title = 'Motor'):\n",
    "    # Get labels for FreeSurfer 'aparc' cortical parcellation with 34 labels/hemi\n",
    "    labels_parc = mne.read_labels_from_annot(subject, parc='aparc',\n",
    "                                            subjects_dir=subjects_dir)\n",
    "    labels_parc = rel_labels\n",
    "    # Average the source estimates within each label of the cortical parcellation\n",
    "    # and each sub-structure contained in the source space.\n",
    "    # When mode = 'mean_flip', this option is used only for the cortical labels.\n",
    "    src = inverse_operator['src']\n",
    "    label_ts = mne.extract_label_time_course(\n",
    "        rel_stcs, labels_parc, src, mode='mean_flip', allow_empty=True,\n",
    "        return_generator=True, verbose=False)\n",
    "\n",
    "    # We compute the connectivity in the alpha band and plot it using a circular\n",
    "    # graph layout\n",
    "    fmin = fmin\n",
    "    fmax = fmax\n",
    "    sfreq = motor_epochs.info['sfreq']  # the sampling frequency\n",
    "    con = spectral_connectivity_epochs(\n",
    "        label_ts, method='pli', mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
    "        fmax=fmax, faverage=True, mt_adaptive=True, n_jobs=5,verbose=False)\n",
    "\n",
    "    # We create a list of Label containing also the sub structures\n",
    "    labels_aseg = mne.get_volume_labels_from_src(src, subject, subjects_dir)\n",
    "    labels = labels_parc + labels_aseg\n",
    "\n",
    "    # read colors\n",
    "    node_colors = [label.color for label in labels]\n",
    "\n",
    "    # We reorder the labels based on their location in the left hemi\n",
    "    label_names = [label.name for label in labels]\n",
    "    lh_labels = [name for name in label_names if name.endswith('lh')]\n",
    "    rh_labels = [name for name in label_names if name.endswith('rh')]\n",
    "\n",
    "    # Get the y-location of the label\n",
    "    label_ypos_lh = list()\n",
    "    for name in lh_labels:\n",
    "        idx = label_names.index(name)\n",
    "        ypos = np.mean(labels[idx].pos[:, 1])\n",
    "        label_ypos_lh.append(ypos)\n",
    "    try:\n",
    "        idx = label_names.index('Brain-Stem')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    else:\n",
    "        ypos = np.mean(labels[idx].pos[:, 1])\n",
    "        lh_labels.append('Brain-Stem')\n",
    "        label_ypos_lh.append(ypos)\n",
    "\n",
    "\n",
    "    # Reorder the labels based on their location\n",
    "    lh_labels = [label for (yp, label) in sorted(zip(label_ypos_lh, lh_labels))]\n",
    "\n",
    "    # For the right hemi\n",
    "    rh_labels = [label[:-2] + 'rh' for label in lh_labels\n",
    "                if label != 'Brain-Stem' and label[:-2] + 'rh' in rh_labels]\n",
    "\n",
    "    # Save the plot order\n",
    "    node_order = lh_labels[::-1] + rh_labels\n",
    "\n",
    "    node_angles = circular_layout(label_names, node_order, start_pos=90,\n",
    "                                group_boundaries=[0, len(label_names) // 2])\n",
    "\n",
    "\n",
    "    # Plot the graph using node colors from the FreeSurfer parcellation. We only\n",
    "    # show the 300 strongest connections.\n",
    "    conmat = con.get_data(output='dense')[:, :, 0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), facecolor='black',\n",
    "                        subplot_kw=dict(polar=True))\n",
    "    plot_connectivity_circle(conmat, label_names, n_lines=300, vmin = 0, vmax = .30,\n",
    "                            node_angles=node_angles, node_colors=node_colors,\n",
    "                            title=f\"All-to-All Connectivity {fig_title} Epochs\", ax=ax)\n",
    "    fig.tight_layout()\n",
    "    fname_fig = f\"{output_dir}/connectivity/{fig_title}_plot_mixed_connect.png\"\n",
    "    fig.savefig(fname_fig, facecolor=fig.get_facecolor())\n",
    "    np.save(f\"{output_dir}/connectivity/{fig_title}_conn\",conmat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11a62095",
   "metadata": {},
   "source": [
    "# NOTE WE LIMIT TO 2000 EPOCHS FOR MEMORY USAGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2086e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m bands \u001b[39m=\u001b[39m [(\u001b[39m4.\u001b[39m,\u001b[39m8.\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTheta\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m8.\u001b[39m,\u001b[39m15.\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mAlpha\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m15.\u001b[39m,\u001b[39m32.\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mBeta\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m32.\u001b[39m, \u001b[39m55.\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGamma\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m      5\u001b[0m cond \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHigh\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m rel_stcs \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39;49mminimum_norm\u001b[39m.\u001b[39;49mapply_inverse_epochs(high_motor_sensor[:\u001b[39m2000\u001b[39;49m], inverse_operator,\n\u001b[1;32m      7\u001b[0m                                 lambda2\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m snr \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m                                 method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39meLORETA\u001b[39;49m\u001b[39m\"\u001b[39;49m, pick_ori\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnormal\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m band \u001b[39min\u001b[39;00m bands:\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mband\u001b[39m\u001b[39m'\u001b[39m, band)\n",
      "File \u001b[0;32m<decorator-gen-492>:10\u001b[0m, in \u001b[0;36mapply_inverse_epochs\u001b[0;34m(epochs, inverse_operator, lambda2, method, label, nave, pick_ori, return_generator, prepared, method_params, use_cps, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/mne/minimum_norm/inverse.py:1291\u001b[0m, in \u001b[0;36mapply_inverse_epochs\u001b[0;34m(epochs, inverse_operator, lambda2, method, label, nave, pick_ori, return_generator, prepared, method_params, use_cps, verbose)\u001b[0m\n\u001b[1;32m   1284\u001b[0m stcs \u001b[39m=\u001b[39m _apply_inverse_epochs_gen(\n\u001b[1;32m   1285\u001b[0m     epochs, inverse_operator, lambda2, method\u001b[39m=\u001b[39mmethod, label\u001b[39m=\u001b[39mlabel,\n\u001b[1;32m   1286\u001b[0m     nave\u001b[39m=\u001b[39mnave, pick_ori\u001b[39m=\u001b[39mpick_ori, verbose\u001b[39m=\u001b[39mverbose, prepared\u001b[39m=\u001b[39mprepared,\n\u001b[1;32m   1287\u001b[0m     method_params\u001b[39m=\u001b[39mmethod_params, use_cps\u001b[39m=\u001b[39muse_cps)\n\u001b[1;32m   1289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_generator:\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# return a list\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     stcs \u001b[39m=\u001b[39m [stc \u001b[39mfor\u001b[39;00m stc \u001b[39min\u001b[39;00m stcs]\n\u001b[1;32m   1293\u001b[0m \u001b[39mreturn\u001b[39;00m stcs\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/mne/minimum_norm/inverse.py:1291\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1284\u001b[0m stcs \u001b[39m=\u001b[39m _apply_inverse_epochs_gen(\n\u001b[1;32m   1285\u001b[0m     epochs, inverse_operator, lambda2, method\u001b[39m=\u001b[39mmethod, label\u001b[39m=\u001b[39mlabel,\n\u001b[1;32m   1286\u001b[0m     nave\u001b[39m=\u001b[39mnave, pick_ori\u001b[39m=\u001b[39mpick_ori, verbose\u001b[39m=\u001b[39mverbose, prepared\u001b[39m=\u001b[39mprepared,\n\u001b[1;32m   1287\u001b[0m     method_params\u001b[39m=\u001b[39mmethod_params, use_cps\u001b[39m=\u001b[39muse_cps)\n\u001b[1;32m   1289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_generator:\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# return a list\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     stcs \u001b[39m=\u001b[39m [stc \u001b[39mfor\u001b[39;00m stc \u001b[39min\u001b[39;00m stcs]\n\u001b[1;32m   1293\u001b[0m \u001b[39mreturn\u001b[39;00m stcs\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/mne/minimum_norm/inverse.py:1222\u001b[0m, in \u001b[0;36m_apply_inverse_epochs_gen\u001b[0;34m(epochs, inverse_operator, lambda2, method, label, nave, pick_ori, prepared, method_params, use_cps, verbose)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         sol \u001b[39m=\u001b[39m (K, e[sel])\n\u001b[1;32m   1221\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1222\u001b[0m         sol \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(K, e[sel])\n\u001b[1;32m   1224\u001b[0m src_type \u001b[39m=\u001b[39m _get_src_type(inverse_operator[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m], vertno)\n\u001b[1;32m   1225\u001b[0m stc \u001b[39m=\u001b[39m _make_stc(sol, vertno, tmin\u001b[39m=\u001b[39mtmin, tstep\u001b[39m=\u001b[39mtstep, subject\u001b[39m=\u001b[39msubject,\n\u001b[1;32m   1226\u001b[0m                 vector\u001b[39m=\u001b[39m(pick_ori \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvector\u001b[39m\u001b[39m'\u001b[39m), source_nn\u001b[39m=\u001b[39msource_nn,\n\u001b[1;32m   1227\u001b[0m                 src_type\u001b[39m=\u001b[39msrc_type)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bands = [(4.,8.,'Theta'), (8.,15.,'Alpha'), (15.,32.,'Beta'), (32., 55., 'Gamma')]\n",
    "\n",
    "cond = 'High'\n",
    "rel_stcs = mne.minimum_norm.apply_inverse_epochs(high_motor_sensor[np.round(np.linspace(0, len(high_motor_sensor)-1, 2000)).astype(int)], inverse_operator,\n",
    "                                lambda2=1.0 / snr ** 2, verbose=False,\n",
    "                                method=\"eLORETA\", pick_ori=\"normal\")\n",
    "for band in bands:\n",
    "    print('band', band)\n",
    "    get_connectivity_plot(rel_stcs,output_dir,fmin = band[0], fmax = band[1], fig_title = f\"{band[2]} {cond} Motor\")\n",
    "\n",
    "cond = 'Low'\n",
    "rel_stcs = mne.minimum_norm.apply_inverse_epochs(low_motor_sensor[np.round(np.linspace(0, len(low_motor_sensor)-1, 2000)).astype(int)], inverse_operator,\n",
    "                                lambda2=1.0 / snr ** 2, verbose=False,\n",
    "                                method=\"eLORETA\", pick_ori=\"normal\")\n",
    "for band in bands:\n",
    "    print('band', band)\n",
    "    get_connectivity_plot(rel_stcs,output_dir,fmin = band[0], fmax = band[1], fig_title = f\"{band[2]} {cond} Motor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e065ad",
   "metadata": {},
   "source": [
    "## Decoding source space data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744c771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mne.decoding import (cross_val_multiscore, LinearModel, SlidingEstimator,\n",
    "                          get_coef)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db9090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 3\n",
    "stcs = mne.minimum_norm.apply_inverse_epochs(motor_epochs[::interval], inverse_operator,\n",
    "                            lambda2=1.0 / snr ** 2, verbose=False,\n",
    "                            method=\"eLORETA\", pick_ori=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aba904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3786bd6576664425be759f66e43a553c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Fitting SlidingEstimator : 0/161 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   22.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   22.8s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.0min remaining:    0.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f7b974dd0f4099a22db8f2963b72a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Fitting SlidingEstimator : 0/161 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.6s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 16.1min remaining:    0.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf9a6f7df844c4a9191e668072c70b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Fitting SlidingEstimator : 0/161 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve source space data into an array\n",
    "X = np.array([stc.lh_data for stc in stcs])  # only keep left hemisphere\n",
    "y = motor_epochs[::interval].metadata.Steer_Wheel_Degree_Encoded # for sparse: motor_epochs[::7]\n",
    "\n",
    "# prepare a series of classifier applied at each time sample\n",
    "clf = make_pipeline(StandardScaler(),  # z-score normalization\n",
    "                    SelectKBest(f_classif, k=500),  # select features for speed\n",
    "                    LinearModel(LogisticRegression(C=1, solver='liblinear')))\n",
    "time_decod = SlidingEstimator(clf, scoring='roc_auc')\n",
    "\n",
    "# Run cross-validated decoding analyses:\n",
    "scores = cross_val_multiscore(time_decod, X, y, cv=5, n_jobs=None)\n",
    "\n",
    "# Plot average decoding scores of 5 splits\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(motor_epochs.times, scores.mean(0), label='score')\n",
    "ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "ax.axvline(0, color='k')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7829687db54201bcc4da14c304bf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Fitting SlidingEstimator : 0/161 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [6.49754236e-12 9.41614838e-12 4.18547000e-11]\n"
     ]
    }
   ],
   "source": [
    "# The fitting needs not be cross validated because the weights are based on\n",
    "# the training sets\n",
    "time_decod.fit(X, y)\n",
    "\n",
    "# Retrieve patterns after inversing the z-score normalization step:\n",
    "patterns = get_coef(time_decod, 'patterns_', inverse_transform=True)\n",
    "\n",
    "stc = stcs[0]  # for convenience, lookup parameters from first stc\n",
    "vertices = [stc.lh_vertno, np.array([], int)]  # empty array for right hemi\n",
    "stc_feat = mne.SourceEstimate(np.abs(patterns), vertices=vertices,\n",
    "                              tmin=stc.tmin, tstep=stc.tstep, subject='fsaverage')\n",
    "\n",
    "brain = stc_feat.plot(views=['lat'], transparent=True, subject='fsaverage',time_unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481f5b0",
   "metadata": {},
   "source": [
    "### CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733f2548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "decoding\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('csp',\n                 CSP({'component_order': 'mutual_info',\n 'cov_est': 'concat',\n 'cov_method_params': None,\n 'log': None,\n 'n_components': 3,\n 'norm_trace': False,\n 'rank': None,\n 'reg': None,\n 'transform_into': 'average_power'}))]) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m csp \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mdecoding\u001b[39m.\u001b[39mCSP(n_components\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, norm_trace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m clf_csp \u001b[39m=\u001b[39m make_pipeline(\n\u001b[1;32m      8\u001b[0m     csp\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m scores \u001b[39m=\u001b[39m cross_val_multiscore(clf_csp, X, y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCSP: \u001b[39m\u001b[39m%0.1f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m scores\u001b[39m.\u001b[39mmean(),))\n",
      "File \u001b[0;32m<decorator-gen-469>:12\u001b[0m, in \u001b[0;36mcross_val_multiscore\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/mne/decoding/base.py:429\u001b[0m, in \u001b[0;36mcross_val_multiscore\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    427\u001b[0m cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    428\u001b[0m cv_iter \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(cv\u001b[39m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m--> 429\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39;49mscoring)\n\u001b[1;32m    430\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m# Note: this parallelization is implemented using MNE Parallel\u001b[39;00m\n\u001b[1;32m    433\u001b[0m parallel, p_func, n_jobs \u001b[39m=\u001b[39m parallel_func(_fit_and_score, n_jobs,\n\u001b[1;32m    434\u001b[0m                                          pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[39m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:450\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 450\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhave a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method. The estimator \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m does not.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m             \u001b[39m%\u001b[39m estimator)\n\u001b[1;32m    454\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(scoring, Iterable):\n\u001b[1;32m    455\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFor evaluating multiple scores, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection.cross_validate instead. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m was passed.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scoring))\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('csp',\n                 CSP({'component_order': 'mutual_info',\n 'cov_est': 'concat',\n 'cov_method_params': None,\n 'log': None,\n 'n_components': 3,\n 'norm_trace': False,\n 'rank': None,\n 'reg': None,\n 'transform_into': 'average_power'}))]) does not."
     ]
    }
   ],
   "source": [
    "# Retrieve source space data into an array\n",
    "print('starting')\n",
    "X = np.array([stc.lh_data for stc in stcs])  # only keep left hemisphere\n",
    "y = np.array(motor_epochs[::interval].metadata.Steer_Wheel_Degree_Encoded)\n",
    "print('decoding')\n",
    "csp = mne.decoding.CSP(n_components=3, norm_trace=False)\n",
    "clf_csp = make_pipeline(\n",
    "    csp\n",
    ")\n",
    "scores = cross_val_multiscore(clf_csp, X, y, cv=5, n_jobs=1)\n",
    "print('CSP: %0.1f%%' % (100 * scores.mean(),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c66fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 10242, 161), (733,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6acb0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [52, 733]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m time_decod \u001b[39m=\u001b[39m SlidingEstimator(clf, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Run cross-validated decoding analyses:\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m scores \u001b[39m=\u001b[39m cross_val_multiscore(time_decod, X, y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Plot average decoding scores of 5 splits\u001b[39;00m\n\u001b[1;32m     14\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<decorator-gen-469>:12\u001b[0m, in \u001b[0;36mcross_val_multiscore\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/mne/decoding/base.py:425\u001b[0m, in \u001b[0;36mcross_val_multiscore\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_split\u001b[39;00m \u001b[39mimport\u001b[39;00m check_cv\n\u001b[1;32m    423\u001b[0m check_scoring \u001b[39m=\u001b[39m _get_check_scoring()\n\u001b[0;32m--> 425\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    427\u001b[0m cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    428\u001b[0m cv_iter \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(cv\u001b[39m.\u001b[39msplit(X, y, groups))\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/sklearn/utils/validation.py:356\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \n\u001b[1;32m    346\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39m    List of objects to ensure sliceability.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 356\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    357\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/mna/lib/python3.10/site-packages/sklearn/utils/validation.py:319\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths])\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [52, 733]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieve source space data into an array\n",
    "X = np.array([stc.lh_data for stc in stcs])\n",
    "y = motor_epochs[::7].metadata.Abs_Steer_Wheel_Degree\n",
    "\n",
    "# prepare a series of classifier applied at each time sample\n",
    "clf = make_pipeline(StandardScaler(),  # z-score normalization\n",
    "                    SelectKBest(f_classif, k=500),  # select features for speed\n",
    "                    LinearModel(LinearRegression()))\n",
    "time_decod = SlidingEstimator(clf, scoring='neg_mean_squared_error')\n",
    "# Run cross-validated decoding analyses:\n",
    "scores = cross_val_multiscore(time_decod, X, y, cv=5, n_jobs=None)\n",
    "\n",
    "# Plot average decoding scores of 5 splits\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(motor_epochs.times, -scores.mean(0), label='score')\n",
    "ax.axhline(mean_squared_error(y,[np.mean(y)]*len(y)), color='k', linestyle='--', label='chance')\n",
    "ax.axvline(0, color='k')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = motor_epochs.metadata.Abs_Steer_Wheel_Degree\n",
    "\n",
    "print(mean_squared_error(y,[np.mean(y)]*len(y),squared=False)), print(np.sqrt(0.025))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583156f0",
   "metadata": {},
   "source": [
    "## Source permutation t-tests with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the inverse operator for use...\n",
      "    Scaled noise and source covariance from nave = 1 to nave = 2697\n",
      "    Created the regularized inverter\n",
      "    Created an SSP operator (subspace dimension = 1)\n",
      "    Created the whitener using a noise covariance matrix with rank 63 (1 small eigenvalues omitted)\n",
      "    Computing optimized source covariance (eLORETA)...\n",
      "        Using independent orientation weights\n",
      "        Fitting up to 20 iterations (this make take a while)...\n",
      "        Converged on iteration 8 (6.8e-07 < 1e-06)\n",
      "        Updating inverse with weighted eigen leads\n",
      "[done]\n",
      "Applying inverse operator to \"0.32 × easy + 0.23 × hard + 0.45 × unknown\"...\n",
      "    Picked 64 channels from the data\n",
      "    Computing inverse...\n",
      "    Eigenleads already weighted ... \n",
      "    Computing residual...\n",
      "    Explained  85.1% variance\n",
      "    Combining the current components...\n",
      "[done]\n",
      "Preparing the inverse operator for use...\n",
      "    Scaled noise and source covariance from nave = 1 to nave = 2429\n",
      "    Created the regularized inverter\n",
      "    Created an SSP operator (subspace dimension = 1)\n",
      "    Created the whitener using a noise covariance matrix with rank 63 (1 small eigenvalues omitted)\n",
      "    Computing optimized source covariance (eLORETA)...\n",
      "        Using independent orientation weights\n",
      "        Fitting up to 20 iterations (this make take a while)...\n",
      "        Converged on iteration 8 (6.8e-07 < 1e-06)\n",
      "        Updating inverse with weighted eigen leads\n",
      "[done]\n",
      "Applying inverse operator to \"0.32 × easy + 0.21 × hard + 0.47 × unknown\"...\n",
      "    Picked 64 channels from the data\n",
      "    Computing inverse...\n",
      "    Eigenleads already weighted ... \n",
      "    Computing residual...\n",
      "    Explained  78.6% variance\n",
      "    Combining the current components...\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "#low_motor.resample(50, npad='auto')\n",
    "condition1 = mne.minimum_norm.apply_inverse(low_motor_sensor.average(), inverse_operator, lambda2, method)\n",
    "#high_motor.resample(50, npad='auto')\n",
    "condition2 = mne.minimum_norm.apply_inverse(high_motor_sensor.average(), inverse_operator, lambda2, method)\n",
    "\n",
    "tmin = condition1.tmin\n",
    "tstep = condition1.tstep * 1000  # convert to milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6984fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "Simulating data for 60 and 60 subjects.\n",
      "Computing adjacency.\n",
      "-- number of adjacent vertices : 20484\n",
      "Clustering.\n",
      "stat_fun(H1): min=0.000000 max=25.034172\n",
      "Running initial clustering …\n",
      "Found 204174 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb90bc466f424febb63b8296f7614cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/49 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mne import spatial_src_adjacency\n",
    "from mne.stats import spatio_temporal_cluster_test, summarize_clusters_stc\n",
    "import scipy\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "n_subjects1, n_subjects2 = 60, 60\n",
    "n_vertices_fsave, n_times = condition1.data.shape\n",
    "print('Simulating data for %d and %d subjects.' % (n_subjects1, n_subjects2))\n",
    "\n",
    "#    Let's make sure our results replicate, so set the seed.\n",
    "np.random.seed(0)\n",
    "X1 = np.random.randn(n_vertices_fsave, n_times, n_subjects1)*10\n",
    "X2 = np.random.randn(n_vertices_fsave, n_times, n_subjects2)*10\n",
    "X1[:, :, :] += condition1.data[:, :, np.newaxis]\n",
    "X2[:, :, :] += 3*condition2.data[:, :, np.newaxis]\n",
    "#    We want to compare the overall activity levels for each subject\n",
    "X1 = np.abs(X1)  # only magnitude\n",
    "X2 = np.abs(X2)  # only magnitude\n",
    "print('Computing adjacency.')\n",
    "adjacency = spatial_src_adjacency(src)\n",
    "\n",
    "#    Note that X needs to be a list of multi-dimensional array of shape\n",
    "#    samples (subjects_k) × time × space, so we permute dimensions\n",
    "X1 = np.transpose(X1, [2, 1, 0])\n",
    "X2 = np.transpose(X2, [2, 1, 0])\n",
    "X = [X1, X2]\n",
    "# Now let's actually do the clustering. This can take a long time...\n",
    "# Here we set the threshold quite high to reduce computation,\n",
    "# and use a very low number of permutations for the same reason.\n",
    "n_permutations = 50 # 50 to test, 1024 ideally\n",
    "p_threshold = .05\n",
    "f_threshold = scipy.stats.distributions.f.ppf(1. - p_threshold / 2.,\n",
    "                                        n_subjects1 - 1, n_subjects2 - 1)\n",
    "print('Clustering.')\n",
    "F_obs, clusters, cluster_p_values, H0 = clu =\\\n",
    "    spatio_temporal_cluster_test(\n",
    "        X, adjacency=adjacency, n_jobs=5, n_permutations=n_permutations,\n",
    "        threshold=f_threshold, buffer_size=None)\n",
    "#    Now select the clusters that are sig. at p < 0.05 (note that this value\n",
    "#    is multiple-comparisons corrected).\n",
    "good_cluster_inds = np.where(cluster_p_values < 0.05 )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c428ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Visualizing clusters.')\n",
    "\n",
    "#    Now let's build a convenient representation of each cluster, where each\n",
    "#    cluster becomes a \"time point\" in the SourceEstimate\n",
    "fsave_vertices = [np.arange(10242), np.arange(10242)]\n",
    "stc_all_cluster_vis = summarize_clusters_stc(clu, tstep=tstep,p_thresh=.05, tmin=-1,\n",
    "                                             vertices=fsave_vertices,\n",
    "                                             subject='fsaverage')\n",
    "\n",
    "#    Let's actually plot the first \"time point\" in the SourceEstimate, which\n",
    "#    shows all the clusters, weighted by duration\n",
    "\n",
    "# blue blobs are for condition A != condition B\n",
    "brain = stc_all_cluster_vis.plot('fsaverage', hemi='both',\n",
    "                                 views='lateral', subjects_dir=subjects_dir,\n",
    "                                 time_label='temporal extent (ms)',\n",
    "                                 clim=dict(kind='value', lims=[0, 1, 40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "mna",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "mna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "141184e5e4b0e37e2d9a38f8289444d097ee1fc9c81aad8d4b3593b95f3db26b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
